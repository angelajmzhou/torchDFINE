{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IMaaAOoFOxOL"
   },
   "source": [
    "# DFINE Tutorial\n",
    "## Overview\n",
    "\n",
    "DFINE, which stands for **Dynamical Flexible Inference for Nonlinear Embeddings**, is a neural network model that is developed to enable flexible inference, whether causally, non-causally, or even in the presence of missing neural observations. To enable flexible inference, a model must achieve all the following operations simultaneously, without the need to retrain a new model or change the inference structure:\n",
    "\n",
    "1\\) Causal inference (filtering) <br>\n",
    "2\\) Non-causal inference (smoothing) <br>\n",
    "3\\) Account for missing observations, which can occur in wireless neural interfaces\n",
    "\n",
    "DFINE achieves flexible inference. Also, DFINE’s inference is recursive and thus computationally efficient. Flexible inference is essential for developing neurotechnology, such as brain-machine interfaces (BMIs).\n",
    "\n",
    "### Model Architecture\n",
    "To achieve flexible inference, DFINE separates the model into jointly trained manifold and dynamic latent factors such that nonlinearity is captured through the manifold factors and the dynamics can be modeled in tractable linear form on this nonlinear manifold. Also, as its training loss, DFINE can use the future-step-ahead neural prediction error because of its flexible inference capability that allows it to efficiently and recursively compute this loss during training.\n",
    "\n",
    "Specifically, we define the two sets of latent factors as follows: 1) Manifold latent factors ${a}_t \\in \\mathbb{R}^{n_a \\times 1}$ and 2) Dynamic latent factors ${x}_t \\in \\mathbb{R}^{n_x \\times 1}$.\n",
    "\n",
    "First, the dynamic latent factors evolve in time with a linear Gaussian model: $\\begin{equation}{x}_{t+1} = A{x}_t + {w}_t\\tag{1}\\end{equation}$ where $A \\in \\mathbb{R}^{n_x \\times n_x}$ is the state transition matrix and ${w}_t \\in \\mathbb{R}^{n_x \\times 1}$ is zero-mean Gaussian noise with covariance matrix $W \\in \\mathbb{R}^{n_x \\times n_x}$. The manifold latent factors ${a}_t$ are related to the dynamic latent factors ${x}_t$ as: $\\begin{equation}{a}_t = C{x}_t + {r}_t\\tag{2}\\end{equation}$ where $C \\in \\mathbb{R}^{n_a \\times n_x}$ is the emission matrix and ${r}_t \\times \\mathbb{R}^{n_a \\times 1}$ is white Gaussian noise with covariance matrix $R \\in \\mathbb{R}^{n_a \\times n_a}$. Equations (1) and (2) form an LDM with learnable parameters $\\psi = \\{ A, C, W, R, {\\mu}_0, \\Lambda_0 \\}$ where ${\\mu}_0$ and $\\Lambda_0$ are the initial estimate and covariance of dynamic latent factors, respectively.\n",
    "\n",
    "Second, to model nonlinear mappings, we used MLP autoencoders to learn the mapping between neural observations ${y}_t$ and manifold latent factors ${a}_t$. We model the decoder part as a nonlinear mapping $f_\\theta(\\cdot)$ from manifold latent factors to neural observations: $\\begin{equation}{y}_t = f_\\theta({a}_t) + {v}_t\\tag{3}\\end{equation}$ where $\\theta$ are parameters and ${v}_t \\in \\mathbb{R}^{n_y \\times 1}$ is a white Gaussian noise with covariance $V \\in \\mathbb{R}^{n_y \\times n_y}$. Equations (1)-(3) together form the generative model.\n",
    "\n",
    "For inference, we also need the mapping from ${y}_t$ to ${a}_t$, which we characterize as: $\\begin{equation}{a}_t = f_\\phi ({y}_t)\\tag{4}\\end{equation}$ where $f_\\phi(\\cdot)$ represents the encoder in the autoencoder structure and is parameterized by another MLP. All equations above are trained together end-to-end, rather than separately. Further, the middle manifold layer in equation (2) explicitly incorporates a\n",
    "stochastic noise variable $r_t$, whose covariance is learned during training, allowing the nonlinearity with respect to the dynamic latent factors to be stochastic in DFINE. To help with robustness to noise and stochasticity during inference, DFINE learns all the stochastic noise distribution parameters during training, which are then explicitly accounted for at inference.\n",
    "\n",
    "\n",
    "### The Inference Problem\n",
    "Using the equations above, we can infer both the manifold and dynamic latent factors from neural observations ${y}_{1:T}$, where $T$ is the total number of time steps for the observations. We use subscript $t|k$ to denote the inferred latent factors at time $t$ given observations up to time $k$, ${y}_{1:k}$. Thus, $t|t$ denotes filtering (causal) inference given ${y}_{1:t}$, $t+k|t$ denotes the $k$-step-ahead prediction given $y_{1:t}$, and $t|T$ denotes smoothing (non-causal) inference given ${y}_{1:T}$.\n",
    "\n",
    "The inference method is shown in Figure 1b in the paper and is as follows. We first directly but statically obtain an initial estimate of ${a}_t$ based on ${y}_t$ with ${\\hat{a}}_t = f_\\phi({y}_t)$ in equation (4), to provide the noisy observations of the dynamical model, that is, ${\\hat{a}}_t$. Having obtained ${\\hat{a}}_t$, we can now use the dynamical part of the model in equations (1) and (2) to infer ${x}_{t|t}$ with Kalman filtering from ${\\hat{a}}_{1:t}$, and infer ${x}_{t|T}$ with Kalman smoothing from ${\\hat{a}}_{1∶T}$. We can then infer the manifold latent factor as ${a}_{t|t} = C{x}_{t|t}$ and ${a}_{t|T} = C{x}_{t|T}$ on the basis of equation (2). Similarly, we can obtain the filtered neural activity ${y}_{t|t}$ and smoothed neural activity ${y}_{t|T}$ using equation (3) as ${y}_{t|t} = f_{\\theta}({a}_{t|t})$ and ${y}_{t|T} = f_{\\theta}({a}_{t|T})$, respectively.\n",
    "\n",
    "To obtain the $k$-step-ahead predicted neural activity ${y}_{t+k|t}$, we first recursively forward predict the dynamic latent factors $k$ time-steps with the Kalman predictor, and obtain ${x}_{t+k|t}$. Then, we can compute the $k$-step-ahead predictions of manifold latent factors and neural observations with ${a}_{t+k|t} = C{x}_{t+k|t}$ and ${y}_{t+k|t} = f_{\\theta}({a}_{t+k|t})$, respectively.\n",
    "\n",
    "### Training Loss Function\n",
    "Having established the DFINE model and its inference, we can learn the model parameters $\\psi, \\theta, \\phi$ by minimizing:  $\\begin{equation}L(\\psi, \\theta, \\phi) = \\sum_{k=1}^K \\sum_{t=1}^{T-k} e({y}_{t+k|t}, {y}_{t+k}) + \\lambda_{reg} L_2 (\\theta, \\phi)\\tag{5}\\end{equation}$ where $K$ denotes the maximum horizon for future-step-ahead prediction, $e(\\cdot, \\cdot)$ denotes the error measure which is taken as mean-squared error (MSE) loss, $L_2(\\cdot)$ is L2 regularization for the autoencoder parameters $\\{\\theta, \\phi\\}$ to prevent overfitting, and $\\lambda_{reg}$ is the L2 regularization loss scale (see config_dfine.py).\n",
    "\n",
    "### Training tips and hyperparameters\n",
    "DFINE does not have many hyperparameters to tune. Yet **it may be necessary to search over a grid of the following hyperparameters to find the best performing ones for a given dataset (especially for L2 regularization loss scale)**:\n",
    "- L2 regularization loss scale, config.loss.scale_l2. For hyperparameter search, you can use a small grid such as [1e-4, 5e-4, 1e-3, 2e-3] after z-scoring the signals (see below).\n",
    "- $K$, future-step-ahead prediction horizon provided as a list, config.loss.steps_ahead\n",
    "- Encoder/decoder architecture, i.e., number of hidden layers and units in each layer, config.model.hidden_layer_list\n",
    "- Setting $n_a$ higher than $n_y$ may lead to overfitting, it's recommended that $n_a \\leq n_y$\n",
    "- As we show in Extended Data Fig. 8, it's recommended to set $n_a = n_x$ to reduce the hyperparameter search complexity\n",
    "\n",
    "For default values of all hyperparameters, please see config_dfine.py. For the future-step-ahead prediction horizon, we used $K=4$, or config.loss.steps_ahead = [1,2,3,4] for DFINE.\n",
    "\n",
    "It is important to note that for neural signals, we performed **z-scoring** which is highly recommended, please see below and time_series_utils.z_score_tensor. Recommendations above for L2 regularization scale are with z-scoring, which can affect the choice of L2 regularization scale.\n",
    "\n",
    "\n",
    "**DFINE is currently implemented for continuous-valued signals**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from itertools import product\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "from config_dfine import get_default_config\n",
    "from trainers.TrainerDFINE import TrainerDFINE\n",
    "from datasets import DFINEDataset\n",
    "from time_series_utils import z_score_tensor, get_nrmse_error\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def set_seed(seed=0):\n",
    "    torch.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "def train_test_split(y, train_ratio, batch_size = 4, shuffleTrain = True):\n",
    "# Split data into training and test datasets\n",
    "    num_trials = y.shape[0]\n",
    "    num_train_trials = int(train_ratio * num_trials)\n",
    "    num_test_trials = num_trials - num_train_trials\n",
    "    train_y = y[:num_train_trials, ...]\n",
    "    test_y = y[num_train_trials:, ...]\n",
    "\n",
    "    # Z-score the observation tensors\n",
    "    train_y_zsc, mean_y, std_y = z_score_tensor(train_y, fit=True)\n",
    "    test_y_zsc, _, _ = z_score_tensor(test_y, mean=mean_y, std=std_y, fit=False)\n",
    "\n",
    "    # Create DFINE dataset objects and torch dataloaders\n",
    "    train_dataset = DFINEDataset(y=train_y_zsc)\n",
    "    test_dataset = DFINEDataset(y=test_y_zsc)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size = batch_size, shuffle= shuffleTrain)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    return train_dataset, test_dataset, train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    \"scale_l2\": [0.0001, 0.001],\n",
    "    \"latent_factors\" :[7,15]\n",
    "}\n",
    "\n",
    "param_combinations = list(product(*param_grid.values()))\n",
    "param_names = list(param_grid.keys())\n",
    "\n",
    "# Store the best configuration\n",
    "best_params = None\n",
    "best_score = float(\"inf\")\n",
    "\n",
    "\n",
    "for subj in range(1, 21):\n",
    "    subj_num = f\"{subj:02}\" \n",
    "    f = h5py.File('GTH_data\\GTH_s' + str(subj_num) + '_decision_power_struct_nobs.mat', 'r')\n",
    "    dataset = list(f[\"power_struct\"][\"highgamma\"][\"powspctrm\"])\n",
    "    labelsset = np.array(f[\"power_struct\"][\"beh\"][\"gambles\"])\n",
    "    \n",
    "    gen_data = f[\"power_struct\"][\"highgamma\"][\"powspctrm\"]\n",
    "    highgamma = np.array(gen_data) # (time, electrodes, trials)\n",
    "    y= np.moveaxis(highgamma,2,0) # (trials, time, channels)\n",
    "    print(\"dim are: \" , y.shape) #(num_seq, num_steps, dim_y)\n",
    "    seed=0\n",
    "    set_seed(seed)\n",
    "    for params in param_combinations:\n",
    "        # Map parameters to config\n",
    "        param_dict = dict(zip(param_names, params))\n",
    "        latent_factors = param_dict[\"latent_factors\"]\n",
    "        scale_l2 = param_dict[\"scale_l2\"]\n",
    "        config = get_default_config()\n",
    "        config.device = 'cuda'\n",
    "        config.model.activation ='tanh'\n",
    "        config.train.num_epochs = 20\n",
    "        config.train.batch_size = 4\n",
    "        config.lr.init = 0.01\n",
    "        config.model.supervise_behv = False\n",
    "        config.seed = seed\n",
    "        config.model.dim_y = y.shape[2]    \n",
    "        config.model.dim_a = latent_factors     #manifold latent factors\n",
    "        config.model.dim_x = latent_factors     #dynamic latent factors (should be same as dim a)\n",
    "        config.loss.scale_l2 = scale_l2\n",
    "        config.model.save_dir = f'./results/neural/subj_{subj_num}_l2_{scale_l2}_nlatent_{latent_factors}'\n",
    "        trainer_load = TrainerDFINE(config=config)\n",
    "    \n",
    "    \n",
    "        labelsset = labelsset.T\n",
    "        behv_mask = torch.tensor(labelsset)\n",
    "        behv_mask = behv_mask.squeeze() \n",
    "    \n",
    "        # Filter trials based on the mask\n",
    "        gamble = y[behv_mask == 1]  # Trials where the mask is 1\n",
    "        no_gamble = y[behv_mask == 0]  # Trials where the mask is 0\n",
    "    \n",
    "        # Output shapes\n",
    "        print(\"Gamble shape:\", gamble.shape)  # (num_seq_1, 5000, 32)\n",
    "        print(\"No Gamble shape:\", no_gamble.shape)  # (num_seq_0, 5000, 32)\n",
    "    \n",
    "        train_dataset, test_dataset, train_loader, test_loader = train_test_split(y, 0.8)\n",
    "    \n",
    "        trainer_load.train(train_loader=train_loader, valid_loader=test_loader)\n",
    "    #CONTINUE TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "   def avg_latent_factor_plot(f, savedir, prefix='gamble', feat_name='x_smooth'):\n",
    "        '''\n",
    "        Creates dynamic latent factor plots during training/validation\n",
    "\n",
    "        Parameters:\n",
    "        ------------\n",
    "        - f: torch.Tensor, shape: (num_seq, num_steps, dim_x/dim_a), Batch of inferred dynamic/manifold latent factors, smoothed/filtered factors can be provided\n",
    "        - epoch: int, Number of epoch for which to create dynamic latent factor plot\n",
    "        - trial_num: int, Trial number to plot\n",
    "        - prefix: str, Plotname prefix to save plots\n",
    "        - feat_name: str, Feature name of y_hat_batch (e.g. y_hat/y_smooth) used in plotname\n",
    "        '''\n",
    "        \n",
    "        # From feat_name, get whether it's manifold or dynamic latent factors\n",
    "        if feat_name[0].lower() == 'x':\n",
    "            feat_name = 'Dynamic'\n",
    "        else:\n",
    "            feat_name = 'Manifold' \n",
    "\n",
    "        # Create the figure and colormap\n",
    "        fig = plt.figure(figsize=(10,8))\n",
    "        num_steps, dim_f = f.shape\n",
    "        color_index = range(num_steps)\n",
    "        color_map = plt.cm.get_cmap('viridis')\n",
    "        \n",
    "        if dim_f > 2:\n",
    "            # Scatter first 3 dimensions of dynamic latent factors \n",
    "            ax = fig.add_subplot(221, projection='3d')\n",
    "            ax_m = ax.scatter(f[:, 0], f[:, 1], f[:, 2], c=color_index, vmin=0, vmax=num_steps, s=35, cmap=color_map)\n",
    "            ax.set_xlabel('Dim 0')\n",
    "            ax.set_ylabel('Dim 1')\n",
    "            ax.set_zlabel('Dim 2')\n",
    "            ax.set_title(f'{feat_name} latent factors in 3D')\n",
    "            fig.colorbar(ax_m)\n",
    "\n",
    "            # Scatter first 2 dimensions of dynamic latent factors, top view\n",
    "            ax = fig.add_subplot(222)\n",
    "            ax_m = ax.scatter(f[:, 0], f[:, 1], c=color_index, vmin=0, vmax=num_steps, s=35, cmap=color_map)\n",
    "            ax.set_xlabel('Dim 0')\n",
    "            ax.set_ylabel('Dim 1')\n",
    "            ax.set_title(f'{feat_name} latent factors from top')\n",
    "            fig.colorbar(ax_m)\n",
    "\n",
    "            # Plot the first dimension of dynamic latent factors\n",
    "            ax = fig.add_subplot(223)\n",
    "            ax.plot(range(num_steps), f[:, 0])\n",
    "            ax.set_xlabel('Time')\n",
    "            ax.set_ylabel('Dim 0')\n",
    "\n",
    "            # Plot the second dimension of dynamic latent factors\n",
    "            ax = fig.add_subplot(224)\n",
    "            ax.plot(range(num_steps), f[:, 1])\n",
    "            ax.set_xlabel('Time')\n",
    "            ax.set_ylabel('Dim 1')\n",
    "\n",
    "        elif dim_f == 2:\n",
    "            # Scatter first 2 dimensions of dynamic latent factors, top view\n",
    "            ax = fig.add_subplot(221)\n",
    "            ax_m = ax.scatter(f[:, 0], f[:, 1], c=color_index, vmin=0, vmax=num_steps, s=35, cmap=color_map)\n",
    "            ax.set_xlabel('Dim 0')\n",
    "            ax.set_ylabel('Dim 1')\n",
    "            ax.set_title(f'{feat_name} latent factors from top')\n",
    "            fig.colorbar(ax_m)\n",
    "\n",
    "            # Plot the first dimension of dynamic latent factors\n",
    "            ax = fig.add_subplot(222)\n",
    "            ax.plot(range(num_steps), f[:, 0])\n",
    "            ax.set_xlabel('Time')\n",
    "            ax.set_ylabel('Dim 0')\n",
    "\n",
    "            # Plot the second dimension of dynamic latent factors\n",
    "            ax = fig.add_subplot(223)\n",
    "            ax.plot(range(num_steps), f[:, 1])\n",
    "            ax.set_xlabel('Time')\n",
    "            ax.set_ylabel('Dim 1')\n",
    "\n",
    "        else:\n",
    "            # Plot the first dimension of dynamic latent factors\n",
    "            ax = fig.add_subplot(111)\n",
    "            ax.plot(range(num_steps), f[:, 0])\n",
    "            ax.set_xlabel('Time')\n",
    "            ax.set_ylabel('Dim 0')\n",
    "        fig.suptitle(f'{feat_name} latent factors info', fontsize=16)\n",
    "        \n",
    "        # Save the plot under plot_save_dir\n",
    "        plot_name = f'{prefix}_{feat_name}_avg.png'\n",
    "        plt.savefig(os.path.join(savedir, \"plots\", plot_name))\n",
    "        plt.close('all')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "def findEuclidianDist(series1, series2):\n",
    "\n",
    "    # Compute Euclidean distances for each time step\n",
    "    distances = torch.sqrt(torch.sum((series1 - series2) ** 2, dim=1))  #sum over dimension tuples\n",
    "\n",
    "    # Overall distance (optional)\n",
    "    total_distance = distances.sum()  # Total distance\n",
    "    mean_distance = distances.mean()  # Total distance\n",
    "\n",
    "    return total_distance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dim are:  (180, 5001, 57)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02/02/2025 02:05:44 PM - DFINE Logger - WARNING - Optimizer and LR scheduler can be loaded only in resume_train mode, else they are re-initialized\n",
      "02/02/2025 02:05:44 PM - DFINE Logger - INFO - Loading model from: ./results/neural/subj_01\\ckpts\\best_loss_ckpt.pth...\n",
      "02/02/2025 02:05:44 PM - DFINE Logger - INFO - Checkpoint succesfully loaded from ./results/neural/subj_01\\ckpts\\best_loss_ckpt.pth!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainerDFINE loaded from: C:\\Users\\angel\\Desktop\\Code\\torchDFINE\\trainers\\TrainerDFINE.py\n",
      "File loaded successfully.\n",
      "g avg dim are:  torch.Size([1001, 3])\n",
      "ng avgdim are:  torch.Size([1001, 3])\n",
      "g avg dim are:  torch.Size([1001, 3])\n",
      "ng avgdim are:  torch.Size([1001, 3])\n",
      "g avg dim are:  torch.Size([1001, 3])\n",
      "ng avgdim are:  torch.Size([1001, 3])\n",
      "g avg dim are:  torch.Size([1001, 3])\n",
      "ng avgdim are:  torch.Size([1001, 3])\n",
      "g avg dim are:  torch.Size([1001, 3])\n",
      "ng avgdim are:  torch.Size([1001, 3])\n",
      "g avg dim are:  torch.Size([1001, 3])\n",
      "ng avgdim are:  torch.Size([1001, 3])\n",
      "g avg dim are:  torch.Size([1001, 3])\n",
      "ng avgdim are:  torch.Size([1001, 3])\n",
      "dim are:  (188, 5001, 60)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02/02/2025 02:05:53 PM - DFINE Logger - WARNING - Optimizer and LR scheduler can be loaded only in resume_train mode, else they are re-initialized\n",
      "02/02/2025 02:05:53 PM - DFINE Logger - INFO - Loading model from: ./results/neural/subj_02\\ckpts\\best_loss_ckpt.pth...\n",
      "02/02/2025 02:05:53 PM - DFINE Logger - INFO - Checkpoint succesfully loaded from ./results/neural/subj_02\\ckpts\\best_loss_ckpt.pth!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainerDFINE loaded from: C:\\Users\\angel\\Desktop\\Code\\torchDFINE\\trainers\\TrainerDFINE.py\n",
      "File loaded successfully.\n",
      "g avg dim are:  torch.Size([1001, 3])\n",
      "ng avgdim are:  torch.Size([1001, 3])\n",
      "g avg dim are:  torch.Size([1001, 3])\n",
      "ng avgdim are:  torch.Size([1001, 3])\n",
      "g avg dim are:  torch.Size([1001, 3])\n",
      "ng avgdim are:  torch.Size([1001, 3])\n",
      "g avg dim are:  torch.Size([1001, 3])\n",
      "ng avgdim are:  torch.Size([1001, 3])\n",
      "g avg dim are:  torch.Size([1001, 3])\n",
      "ng avgdim are:  torch.Size([1001, 3])\n",
      "g avg dim are:  torch.Size([1001, 3])\n",
      "ng avgdim are:  torch.Size([1001, 3])\n",
      "g avg dim are:  torch.Size([1001, 3])\n",
      "ng avgdim are:  torch.Size([1001, 3])\n",
      "dim are:  (177, 5001, 250)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02/02/2025 02:06:17 PM - DFINE Logger - WARNING - Optimizer and LR scheduler can be loaded only in resume_train mode, else they are re-initialized\n",
      "02/02/2025 02:06:17 PM - DFINE Logger - INFO - Loading model from: ./results/neural/subj_03\\ckpts\\best_loss_ckpt.pth...\n",
      "02/02/2025 02:06:17 PM - DFINE Logger - INFO - Checkpoint succesfully loaded from ./results/neural/subj_03\\ckpts\\best_loss_ckpt.pth!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainerDFINE loaded from: C:\\Users\\angel\\Desktop\\Code\\torchDFINE\\trainers\\TrainerDFINE.py\n",
      "File loaded successfully.\n",
      "g avg dim are:  torch.Size([1001, 3])\n",
      "ng avgdim are:  torch.Size([1001, 3])\n",
      "g avg dim are:  torch.Size([1001, 3])\n",
      "ng avgdim are:  torch.Size([1001, 3])\n",
      "g avg dim are:  torch.Size([1001, 3])\n",
      "ng avgdim are:  torch.Size([1001, 3])\n",
      "g avg dim are:  torch.Size([1001, 3])\n",
      "ng avgdim are:  torch.Size([1001, 3])\n",
      "g avg dim are:  torch.Size([1001, 3])\n",
      "ng avgdim are:  torch.Size([1001, 3])\n",
      "g avg dim are:  torch.Size([1001, 3])\n",
      "ng avgdim are:  torch.Size([1001, 3])\n",
      "g avg dim are:  torch.Size([1001, 3])\n",
      "ng avgdim are:  torch.Size([1001, 3])\n",
      "dim are:  (194, 5001, 101)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02/02/2025 02:06:30 PM - DFINE Logger - WARNING - Optimizer and LR scheduler can be loaded only in resume_train mode, else they are re-initialized\n",
      "02/02/2025 02:06:30 PM - DFINE Logger - INFO - Loading model from: ./results/neural/subj_04\\ckpts\\best_loss_ckpt.pth...\n",
      "02/02/2025 02:06:30 PM - DFINE Logger - INFO - Checkpoint succesfully loaded from ./results/neural/subj_04\\ckpts\\best_loss_ckpt.pth!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainerDFINE loaded from: C:\\Users\\angel\\Desktop\\Code\\torchDFINE\\trainers\\TrainerDFINE.py\n",
      "File loaded successfully.\n",
      "g avg dim are:  torch.Size([1001, 3])\n",
      "ng avgdim are:  torch.Size([1001, 3])\n",
      "g avg dim are:  torch.Size([1001, 3])\n",
      "ng avgdim are:  torch.Size([1001, 3])\n",
      "g avg dim are:  torch.Size([1001, 3])\n",
      "ng avgdim are:  torch.Size([1001, 3])\n",
      "g avg dim are:  torch.Size([1001, 3])\n",
      "ng avgdim are:  torch.Size([1001, 3])\n",
      "g avg dim are:  torch.Size([1001, 3])\n",
      "ng avgdim are:  torch.Size([1001, 3])\n",
      "g avg dim are:  torch.Size([1001, 3])\n",
      "ng avgdim are:  torch.Size([1001, 3])\n",
      "g avg dim are:  torch.Size([1001, 3])\n",
      "ng avgdim are:  torch.Size([1001, 3])\n",
      "dim are:  (176, 5001, 143)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02/02/2025 02:06:44 PM - DFINE Logger - WARNING - Optimizer and LR scheduler can be loaded only in resume_train mode, else they are re-initialized\n",
      "02/02/2025 02:06:44 PM - DFINE Logger - INFO - Loading model from: ./results/neural/subj_05\\ckpts\\best_loss_ckpt.pth...\n",
      "02/02/2025 02:06:44 PM - DFINE Logger - INFO - Checkpoint succesfully loaded from ./results/neural/subj_05\\ckpts\\best_loss_ckpt.pth!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainerDFINE loaded from: C:\\Users\\angel\\Desktop\\Code\\torchDFINE\\trainers\\TrainerDFINE.py\n",
      "File loaded successfully.\n",
      "g avg dim are:  torch.Size([1001, 3])\n",
      "ng avgdim are:  torch.Size([1001, 3])\n",
      "g avg dim are:  torch.Size([1001, 3])\n",
      "ng avgdim are:  torch.Size([1001, 3])\n",
      "g avg dim are:  torch.Size([1001, 3])\n",
      "ng avgdim are:  torch.Size([1001, 3])\n",
      "g avg dim are:  torch.Size([1001, 3])\n",
      "ng avgdim are:  torch.Size([1001, 3])\n",
      "g avg dim are:  torch.Size([1001, 3])\n",
      "ng avgdim are:  torch.Size([1001, 3])\n",
      "g avg dim are:  torch.Size([1001, 3])\n",
      "ng avgdim are:  torch.Size([1001, 3])\n",
      "g avg dim are:  torch.Size([1001, 3])\n",
      "ng avgdim are:  torch.Size([1001, 3])\n",
      "dim are:  (187, 5001, 96)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02/02/2025 02:06:58 PM - DFINE Logger - WARNING - Optimizer and LR scheduler can be loaded only in resume_train mode, else they are re-initialized\n",
      "02/02/2025 02:06:58 PM - DFINE Logger - INFO - Loading model from: ./results/neural/subj_06\\ckpts\\best_loss_ckpt.pth...\n",
      "02/02/2025 02:06:58 PM - DFINE Logger - INFO - Checkpoint succesfully loaded from ./results/neural/subj_06\\ckpts\\best_loss_ckpt.pth!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainerDFINE loaded from: C:\\Users\\angel\\Desktop\\Code\\torchDFINE\\trainers\\TrainerDFINE.py\n",
      "File loaded successfully.\n",
      "g avg dim are:  torch.Size([1001, 3])\n",
      "ng avgdim are:  torch.Size([1001, 3])\n",
      "g avg dim are:  torch.Size([1001, 3])\n",
      "ng avgdim are:  torch.Size([1001, 3])\n",
      "g avg dim are:  torch.Size([1001, 3])\n",
      "ng avgdim are:  torch.Size([1001, 3])\n",
      "g avg dim are:  torch.Size([1001, 3])\n",
      "ng avgdim are:  torch.Size([1001, 3])\n",
      "g avg dim are:  torch.Size([1001, 3])\n",
      "ng avgdim are:  torch.Size([1001, 3])\n",
      "g avg dim are:  torch.Size([1001, 3])\n",
      "ng avgdim are:  torch.Size([1001, 3])\n",
      "g avg dim are:  torch.Size([1001, 3])\n",
      "ng avgdim are:  torch.Size([1001, 3])\n",
      "dim are:  (181, 5001, 91)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02/02/2025 02:07:10 PM - DFINE Logger - WARNING - Optimizer and LR scheduler can be loaded only in resume_train mode, else they are re-initialized\n",
      "02/02/2025 02:07:10 PM - DFINE Logger - INFO - Loading model from: ./results/neural/subj_07\\ckpts\\best_loss_ckpt.pth...\n",
      "02/02/2025 02:07:10 PM - DFINE Logger - INFO - Checkpoint succesfully loaded from ./results/neural/subj_07\\ckpts\\best_loss_ckpt.pth!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainerDFINE loaded from: C:\\Users\\angel\\Desktop\\Code\\torchDFINE\\trainers\\TrainerDFINE.py\n",
      "File loaded successfully.\n",
      "g avg dim are:  torch.Size([1001, 3])\n",
      "ng avgdim are:  torch.Size([1001, 3])\n",
      "g avg dim are:  torch.Size([1001, 3])\n",
      "ng avgdim are:  torch.Size([1001, 3])\n",
      "g avg dim are:  torch.Size([1001, 3])\n",
      "ng avgdim are:  torch.Size([1001, 3])\n",
      "g avg dim are:  torch.Size([1001, 3])\n",
      "ng avgdim are:  torch.Size([1001, 3])\n",
      "g avg dim are:  torch.Size([1001, 3])\n",
      "ng avgdim are:  torch.Size([1001, 3])\n",
      "g avg dim are:  torch.Size([1001, 3])\n",
      "ng avgdim are:  torch.Size([1001, 3])\n",
      "g avg dim are:  torch.Size([1001, 3])\n",
      "ng avgdim are:  torch.Size([1001, 3])\n",
      "dim are:  (196, 5001, 108)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02/02/2025 02:07:26 PM - DFINE Logger - WARNING - Optimizer and LR scheduler can be loaded only in resume_train mode, else they are re-initialized\n",
      "02/02/2025 02:07:26 PM - DFINE Logger - INFO - Loading model from: ./results/neural/subj_08\\ckpts\\best_loss_ckpt.pth...\n",
      "02/02/2025 02:07:26 PM - DFINE Logger - INFO - Checkpoint succesfully loaded from ./results/neural/subj_08\\ckpts\\best_loss_ckpt.pth!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainerDFINE loaded from: C:\\Users\\angel\\Desktop\\Code\\torchDFINE\\trainers\\TrainerDFINE.py\n",
      "File loaded successfully.\n",
      "g avg dim are:  torch.Size([1001, 3])\n",
      "ng avgdim are:  torch.Size([1001, 3])\n",
      "g avg dim are:  torch.Size([1001, 3])\n",
      "ng avgdim are:  torch.Size([1001, 3])\n",
      "g avg dim are:  torch.Size([1001, 3])\n",
      "ng avgdim are:  torch.Size([1001, 3])\n",
      "g avg dim are:  torch.Size([1001, 3])\n",
      "ng avgdim are:  torch.Size([1001, 3])\n",
      "g avg dim are:  torch.Size([1001, 3])\n",
      "ng avgdim are:  torch.Size([1001, 3])\n",
      "g avg dim are:  torch.Size([1001, 3])\n",
      "ng avgdim are:  torch.Size([1001, 3])\n",
      "g avg dim are:  torch.Size([1001, 3])\n",
      "ng avgdim are:  torch.Size([1001, 3])\n",
      "dim are:  (200, 5001, 61)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02/02/2025 02:07:37 PM - DFINE Logger - WARNING - Optimizer and LR scheduler can be loaded only in resume_train mode, else they are re-initialized\n",
      "02/02/2025 02:07:37 PM - DFINE Logger - INFO - Loading model from: ./results/neural/subj_09\\ckpts\\best_loss_ckpt.pth...\n",
      "02/02/2025 02:07:37 PM - DFINE Logger - INFO - Checkpoint succesfully loaded from ./results/neural/subj_09\\ckpts\\best_loss_ckpt.pth!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainerDFINE loaded from: C:\\Users\\angel\\Desktop\\Code\\torchDFINE\\trainers\\TrainerDFINE.py\n",
      "File loaded successfully.\n",
      "g avg dim are:  torch.Size([1001, 3])\n",
      "ng avgdim are:  torch.Size([1001, 3])\n",
      "g avg dim are:  torch.Size([1001, 3])\n",
      "ng avgdim are:  torch.Size([1001, 3])\n",
      "g avg dim are:  torch.Size([1001, 3])\n",
      "ng avgdim are:  torch.Size([1001, 3])\n",
      "g avg dim are:  torch.Size([1001, 3])\n",
      "ng avgdim are:  torch.Size([1001, 3])\n",
      "g avg dim are:  torch.Size([1001, 3])\n",
      "ng avgdim are:  torch.Size([1001, 3])\n",
      "g avg dim are:  torch.Size([1001, 3])\n",
      "ng avgdim are:  torch.Size([1001, 3])\n",
      "g avg dim are:  torch.Size([1001, 3])\n",
      "ng avgdim are:  torch.Size([1001, 3])\n",
      "dim are:  (195, 5001, 67)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02/02/2025 02:07:49 PM - DFINE Logger - WARNING - Optimizer and LR scheduler can be loaded only in resume_train mode, else they are re-initialized\n",
      "02/02/2025 02:07:49 PM - DFINE Logger - INFO - Loading model from: ./results/neural/subj_10\\ckpts\\best_loss_ckpt.pth...\n",
      "02/02/2025 02:07:49 PM - DFINE Logger - INFO - Checkpoint succesfully loaded from ./results/neural/subj_10\\ckpts\\best_loss_ckpt.pth!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainerDFINE loaded from: C:\\Users\\angel\\Desktop\\Code\\torchDFINE\\trainers\\TrainerDFINE.py\n",
      "File loaded successfully.\n",
      "g avg dim are:  torch.Size([1001, 3])\n",
      "ng avgdim are:  torch.Size([1001, 3])\n",
      "g avg dim are:  torch.Size([1001, 3])\n",
      "ng avgdim are:  torch.Size([1001, 3])\n",
      "g avg dim are:  torch.Size([1001, 3])\n",
      "ng avgdim are:  torch.Size([1001, 3])\n",
      "g avg dim are:  torch.Size([1001, 3])\n",
      "ng avgdim are:  torch.Size([1001, 3])\n",
      "g avg dim are:  torch.Size([1001, 3])\n",
      "ng avgdim are:  torch.Size([1001, 3])\n",
      "g avg dim are:  torch.Size([1001, 3])\n",
      "ng avgdim are:  torch.Size([1001, 3])\n",
      "g avg dim are:  torch.Size([1001, 3])\n",
      "ng avgdim are:  torch.Size([1001, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02/02/2025 02:07:58 PM - DFINE Logger - WARNING - Optimizer and LR scheduler can be loaded only in resume_train mode, else they are re-initialized\n",
      "02/02/2025 02:07:58 PM - DFINE Logger - INFO - Loading model from: ./results/neural/subj_11\\ckpts\\best_loss_ckpt.pth...\n",
      "02/02/2025 02:07:58 PM - DFINE Logger - INFO - Checkpoint succesfully loaded from ./results/neural/subj_11\\ckpts\\best_loss_ckpt.pth!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dim are:  (74, 5001, 82)\n",
      "TrainerDFINE loaded from: C:\\Users\\angel\\Desktop\\Code\\torchDFINE\\trainers\\TrainerDFINE.py\n",
      "File loaded successfully.\n",
      "g avg dim are:  torch.Size([1001, 3])\n",
      "ng avgdim are:  torch.Size([1001, 3])\n",
      "g avg dim are:  torch.Size([1001, 3])\n",
      "ng avgdim are:  torch.Size([1001, 3])\n",
      "g avg dim are:  torch.Size([1001, 3])\n",
      "ng avgdim are:  torch.Size([1001, 3])\n",
      "g avg dim are:  torch.Size([1001, 3])\n",
      "ng avgdim are:  torch.Size([1001, 3])\n",
      "g avg dim are:  torch.Size([1001, 3])\n",
      "ng avgdim are:  torch.Size([1001, 3])\n",
      "g avg dim are:  torch.Size([1001, 3])\n",
      "ng avgdim are:  torch.Size([1001, 3])\n",
      "g avg dim are:  torch.Size([1001, 3])\n",
      "ng avgdim are:  torch.Size([1001, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02/02/2025 02:08:03 PM - DFINE Logger - WARNING - Optimizer and LR scheduler can be loaded only in resume_train mode, else they are re-initialized\n",
      "02/02/2025 02:08:03 PM - DFINE Logger - INFO - Loading model from: ./results/neural/subj_12\\ckpts\\best_loss_ckpt.pth...\n",
      "02/02/2025 02:08:03 PM - DFINE Logger - INFO - Checkpoint succesfully loaded from ./results/neural/subj_12\\ckpts\\best_loss_ckpt.pth!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dim are:  (38, 5001, 73)\n",
      "TrainerDFINE loaded from: C:\\Users\\angel\\Desktop\\Code\\torchDFINE\\trainers\\TrainerDFINE.py\n",
      "File loaded successfully.\n",
      "g avg dim are:  torch.Size([1001, 3])\n",
      "ng avgdim are:  torch.Size([1001, 3])\n",
      "g avg dim are:  torch.Size([1001, 3])\n",
      "ng avgdim are:  torch.Size([1001, 3])\n",
      "g avg dim are:  torch.Size([1001, 3])\n",
      "ng avgdim are:  torch.Size([1001, 3])\n",
      "g avg dim are:  torch.Size([1001, 3])\n",
      "ng avgdim are:  torch.Size([1001, 3])\n",
      "g avg dim are:  torch.Size([1001, 3])\n",
      "ng avgdim are:  torch.Size([1001, 3])\n",
      "g avg dim are:  torch.Size([1001, 3])\n",
      "ng avgdim are:  torch.Size([1001, 3])\n",
      "g avg dim are:  torch.Size([1001, 3])\n",
      "ng avgdim are:  torch.Size([1001, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02/02/2025 02:08:12 PM - DFINE Logger - WARNING - Optimizer and LR scheduler can be loaded only in resume_train mode, else they are re-initialized\n",
      "02/02/2025 02:08:12 PM - DFINE Logger - INFO - Loading model from: ./results/neural/subj_13\\ckpts\\best_loss_ckpt.pth...\n",
      "02/02/2025 02:08:12 PM - DFINE Logger - INFO - Checkpoint succesfully loaded from ./results/neural/subj_13\\ckpts\\best_loss_ckpt.pth!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dim are:  (200, 5001, 32)\n",
      "TrainerDFINE loaded from: C:\\Users\\angel\\Desktop\\Code\\torchDFINE\\trainers\\TrainerDFINE.py\n",
      "File loaded successfully.\n",
      "g avg dim are:  torch.Size([1001, 3])\n",
      "ng avgdim are:  torch.Size([1001, 3])\n",
      "g avg dim are:  torch.Size([1001, 3])\n",
      "ng avgdim are:  torch.Size([1001, 3])\n",
      "g avg dim are:  torch.Size([1001, 3])\n",
      "ng avgdim are:  torch.Size([1001, 3])\n",
      "g avg dim are:  torch.Size([1001, 3])\n",
      "ng avgdim are:  torch.Size([1001, 3])\n",
      "g avg dim are:  torch.Size([1001, 3])\n",
      "ng avgdim are:  torch.Size([1001, 3])\n",
      "g avg dim are:  torch.Size([1001, 3])\n",
      "ng avgdim are:  torch.Size([1001, 3])\n",
      "g avg dim are:  torch.Size([1001, 3])\n",
      "ng avgdim are:  torch.Size([1001, 3])\n",
      "dim are:  (172, 5001, 68)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02/02/2025 02:08:24 PM - DFINE Logger - WARNING - Optimizer and LR scheduler can be loaded only in resume_train mode, else they are re-initialized\n",
      "02/02/2025 02:08:24 PM - DFINE Logger - INFO - Loading model from: ./results/neural/subj_14\\ckpts\\best_loss_ckpt.pth...\n",
      "02/02/2025 02:08:24 PM - DFINE Logger - INFO - Checkpoint succesfully loaded from ./results/neural/subj_14\\ckpts\\best_loss_ckpt.pth!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainerDFINE loaded from: C:\\Users\\angel\\Desktop\\Code\\torchDFINE\\trainers\\TrainerDFINE.py\n",
      "File loaded successfully.\n",
      "g avg dim are:  torch.Size([1001, 3])\n",
      "ng avgdim are:  torch.Size([1001, 3])\n",
      "g avg dim are:  torch.Size([1001, 3])\n",
      "ng avgdim are:  torch.Size([1001, 3])\n",
      "g avg dim are:  torch.Size([1001, 3])\n",
      "ng avgdim are:  torch.Size([1001, 3])\n",
      "g avg dim are:  torch.Size([1001, 3])\n",
      "ng avgdim are:  torch.Size([1001, 3])\n",
      "g avg dim are:  torch.Size([1001, 3])\n",
      "ng avgdim are:  torch.Size([1001, 3])\n",
      "g avg dim are:  torch.Size([1001, 3])\n",
      "ng avgdim are:  torch.Size([1001, 3])\n",
      "g avg dim are:  torch.Size([1001, 3])\n",
      "ng avgdim are:  torch.Size([1001, 3])\n",
      "dim are:  (172, 5001, 71)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02/02/2025 02:08:34 PM - DFINE Logger - WARNING - Optimizer and LR scheduler can be loaded only in resume_train mode, else they are re-initialized\n",
      "02/02/2025 02:08:34 PM - DFINE Logger - INFO - Loading model from: ./results/neural/subj_15\\ckpts\\best_loss_ckpt.pth...\n",
      "02/02/2025 02:08:34 PM - DFINE Logger - INFO - Checkpoint succesfully loaded from ./results/neural/subj_15\\ckpts\\best_loss_ckpt.pth!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainerDFINE loaded from: C:\\Users\\angel\\Desktop\\Code\\torchDFINE\\trainers\\TrainerDFINE.py\n",
      "File loaded successfully.\n",
      "g avg dim are:  torch.Size([1001, 3])\n",
      "ng avgdim are:  torch.Size([1001, 3])\n",
      "g avg dim are:  torch.Size([1001, 3])\n",
      "ng avgdim are:  torch.Size([1001, 3])\n",
      "g avg dim are:  torch.Size([1001, 3])\n",
      "ng avgdim are:  torch.Size([1001, 3])\n",
      "g avg dim are:  torch.Size([1001, 3])\n",
      "ng avgdim are:  torch.Size([1001, 3])\n",
      "g avg dim are:  torch.Size([1001, 3])\n",
      "ng avgdim are:  torch.Size([1001, 3])\n",
      "g avg dim are:  torch.Size([1001, 3])\n",
      "ng avgdim are:  torch.Size([1001, 3])\n",
      "g avg dim are:  torch.Size([1001, 3])\n",
      "ng avgdim are:  torch.Size([1001, 3])\n",
      "dim are:  (200, 5001, 59)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02/02/2025 02:08:44 PM - DFINE Logger - WARNING - Optimizer and LR scheduler can be loaded only in resume_train mode, else they are re-initialized\n",
      "02/02/2025 02:08:44 PM - DFINE Logger - INFO - Loading model from: ./results/neural/subj_16\\ckpts\\best_loss_ckpt.pth...\n",
      "02/02/2025 02:08:44 PM - DFINE Logger - INFO - Checkpoint succesfully loaded from ./results/neural/subj_16\\ckpts\\best_loss_ckpt.pth!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainerDFINE loaded from: C:\\Users\\angel\\Desktop\\Code\\torchDFINE\\trainers\\TrainerDFINE.py\n",
      "File loaded successfully.\n",
      "g avg dim are:  torch.Size([1001, 3])\n",
      "ng avgdim are:  torch.Size([1001, 3])\n",
      "g avg dim are:  torch.Size([1001, 3])\n",
      "ng avgdim are:  torch.Size([1001, 3])\n",
      "g avg dim are:  torch.Size([1001, 3])\n",
      "ng avgdim are:  torch.Size([1001, 3])\n",
      "g avg dim are:  torch.Size([1001, 3])\n",
      "ng avgdim are:  torch.Size([1001, 3])\n",
      "g avg dim are:  torch.Size([1001, 3])\n",
      "ng avgdim are:  torch.Size([1001, 3])\n",
      "g avg dim are:  torch.Size([1001, 3])\n",
      "ng avgdim are:  torch.Size([1001, 3])\n",
      "g avg dim are:  torch.Size([1001, 3])\n",
      "ng avgdim are:  torch.Size([1001, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02/02/2025 02:08:52 PM - DFINE Logger - WARNING - Optimizer and LR scheduler can be loaded only in resume_train mode, else they are re-initialized\n",
      "02/02/2025 02:08:52 PM - DFINE Logger - INFO - Loading model from: ./results/neural/subj_17\\ckpts\\best_loss_ckpt.pth...\n",
      "02/02/2025 02:08:52 PM - DFINE Logger - INFO - Checkpoint succesfully loaded from ./results/neural/subj_17\\ckpts\\best_loss_ckpt.pth!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dim are:  (54, 5001, 84)\n",
      "TrainerDFINE loaded from: C:\\Users\\angel\\Desktop\\Code\\torchDFINE\\trainers\\TrainerDFINE.py\n",
      "File loaded successfully.\n",
      "g avg dim are:  torch.Size([1001, 3])\n",
      "ng avgdim are:  torch.Size([1001, 3])\n",
      "g avg dim are:  torch.Size([1001, 3])\n",
      "ng avgdim are:  torch.Size([1001, 3])\n",
      "g avg dim are:  torch.Size([1001, 3])\n",
      "ng avgdim are:  torch.Size([1001, 3])\n",
      "g avg dim are:  torch.Size([1001, 3])\n",
      "ng avgdim are:  torch.Size([1001, 3])\n",
      "g avg dim are:  torch.Size([1001, 3])\n",
      "ng avgdim are:  torch.Size([1001, 3])\n",
      "g avg dim are:  torch.Size([1001, 3])\n",
      "ng avgdim are:  torch.Size([1001, 3])\n",
      "g avg dim are:  torch.Size([1001, 3])\n",
      "ng avgdim are:  torch.Size([1001, 3])\n",
      "dim are:  (168, 5001, 97)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02/02/2025 02:09:05 PM - DFINE Logger - WARNING - Optimizer and LR scheduler can be loaded only in resume_train mode, else they are re-initialized\n",
      "02/02/2025 02:09:05 PM - DFINE Logger - INFO - Loading model from: ./results/neural/subj_18\\ckpts\\best_loss_ckpt.pth...\n",
      "02/02/2025 02:09:05 PM - DFINE Logger - INFO - Checkpoint succesfully loaded from ./results/neural/subj_18\\ckpts\\best_loss_ckpt.pth!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainerDFINE loaded from: C:\\Users\\angel\\Desktop\\Code\\torchDFINE\\trainers\\TrainerDFINE.py\n",
      "File loaded successfully.\n",
      "g avg dim are:  torch.Size([1001, 3])\n",
      "ng avgdim are:  torch.Size([1001, 3])\n",
      "g avg dim are:  torch.Size([1001, 3])\n",
      "ng avgdim are:  torch.Size([1001, 3])\n",
      "g avg dim are:  torch.Size([1001, 3])\n",
      "ng avgdim are:  torch.Size([1001, 3])\n",
      "g avg dim are:  torch.Size([1001, 3])\n",
      "ng avgdim are:  torch.Size([1001, 3])\n",
      "g avg dim are:  torch.Size([1001, 3])\n",
      "ng avgdim are:  torch.Size([1001, 3])\n",
      "g avg dim are:  torch.Size([1001, 3])\n",
      "ng avgdim are:  torch.Size([1001, 3])\n",
      "g avg dim are:  torch.Size([1001, 3])\n",
      "ng avgdim are:  torch.Size([1001, 3])\n",
      "dim are:  (197, 5001, 122)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02/02/2025 02:09:23 PM - DFINE Logger - WARNING - Optimizer and LR scheduler can be loaded only in resume_train mode, else they are re-initialized\n",
      "02/02/2025 02:09:23 PM - DFINE Logger - INFO - Loading model from: ./results/neural/subj_19\\ckpts\\best_loss_ckpt.pth...\n",
      "02/02/2025 02:09:23 PM - DFINE Logger - INFO - Checkpoint succesfully loaded from ./results/neural/subj_19\\ckpts\\best_loss_ckpt.pth!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainerDFINE loaded from: C:\\Users\\angel\\Desktop\\Code\\torchDFINE\\trainers\\TrainerDFINE.py\n",
      "File loaded successfully.\n",
      "g avg dim are:  torch.Size([1001, 3])\n",
      "ng avgdim are:  torch.Size([1001, 3])\n",
      "g avg dim are:  torch.Size([1001, 3])\n",
      "ng avgdim are:  torch.Size([1001, 3])\n",
      "g avg dim are:  torch.Size([1001, 3])\n",
      "ng avgdim are:  torch.Size([1001, 3])\n",
      "g avg dim are:  torch.Size([1001, 3])\n",
      "ng avgdim are:  torch.Size([1001, 3])\n",
      "g avg dim are:  torch.Size([1001, 3])\n",
      "ng avgdim are:  torch.Size([1001, 3])\n",
      "g avg dim are:  torch.Size([1001, 3])\n",
      "ng avgdim are:  torch.Size([1001, 3])\n",
      "g avg dim are:  torch.Size([1001, 3])\n",
      "ng avgdim are:  torch.Size([1001, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02/02/2025 02:09:32 PM - DFINE Logger - WARNING - Optimizer and LR scheduler can be loaded only in resume_train mode, else they are re-initialized\n",
      "02/02/2025 02:09:32 PM - DFINE Logger - INFO - Loading model from: ./results/neural/subj_20\\ckpts\\best_loss_ckpt.pth...\n",
      "02/02/2025 02:09:32 PM - DFINE Logger - INFO - Checkpoint succesfully loaded from ./results/neural/subj_20\\ckpts\\best_loss_ckpt.pth!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dim are:  (139, 5001, 34)\n",
      "TrainerDFINE loaded from: C:\\Users\\angel\\Desktop\\Code\\torchDFINE\\trainers\\TrainerDFINE.py\n",
      "File loaded successfully.\n",
      "g avg dim are:  torch.Size([1001, 3])\n",
      "ng avgdim are:  torch.Size([1001, 3])\n",
      "g avg dim are:  torch.Size([1001, 3])\n",
      "ng avgdim are:  torch.Size([1001, 3])\n",
      "g avg dim are:  torch.Size([1001, 3])\n",
      "ng avgdim are:  torch.Size([1001, 3])\n",
      "g avg dim are:  torch.Size([1001, 3])\n",
      "ng avgdim are:  torch.Size([1001, 3])\n",
      "g avg dim are:  torch.Size([1001, 3])\n",
      "ng avgdim are:  torch.Size([1001, 3])\n",
      "g avg dim are:  torch.Size([1001, 3])\n",
      "ng avgdim are:  torch.Size([1001, 3])\n",
      "g avg dim are:  torch.Size([1001, 3])\n",
      "ng avgdim are:  torch.Size([1001, 3])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    \"scale_l2\": [0.0001, 0.001],\n",
    "    \"latent_factors\" :[15,30]\n",
    "}\n",
    "\n",
    "param_combinations = list(product(*param_grid.values()))\n",
    "param_names = list(param_grid.keys())\n",
    "\n",
    "# Store the best configuration\n",
    "best_params = None\n",
    "best_score = float(\"inf\")\n",
    "\n",
    "\n",
    "for subj in range(1, 21):\n",
    "    subj_num = f\"{subj:02}\" \n",
    "    f = h5py.File('GTH_data\\GTH_s' + str(subj_num) + '_decision_power_struct_nobs.mat', 'r')\n",
    "    dataset = list(f[\"power_struct\"][\"highgamma\"][\"powspctrm\"])\n",
    "    labelsset = np.array(f[\"power_struct\"][\"beh\"][\"gambles\"])\n",
    "    \n",
    "    gen_data = f[\"power_struct\"][\"highgamma\"][\"powspctrm\"]\n",
    "    highgamma = np.array(gen_data) # (time, electrodes, trials)\n",
    "    y= np.moveaxis(highgamma,2,0) # (trials, time, channels)\n",
    "    print(\"dim are: \" , y.shape) #(num_seq, num_steps, dim_y)\n",
    "\n",
    "    labelsset = labelsset.T\n",
    "    behv_mask = torch.tensor(labelsset)\n",
    "    behv_mask = behv_mask.squeeze() \n",
    "\n",
    "    gamble = y[behv_mask == 1]\n",
    "    no_gamble = y[behv_mask == 0] \n",
    "    seed=0\n",
    "    set_seed(seed)\n",
    "    for params in param_combinations:\n",
    "        # Map parameters to config\n",
    "        param_dict = dict(zip(param_names, params))\n",
    "        latent_factors = param_dict[\"latent_factors\"]\n",
    "        scale_l2 = param_dict[\"scale_l2\"]\n",
    "        config = get_default_config()\n",
    "        config.device = 'cuda'\n",
    "        config.model.activation ='tanh'\n",
    "        config.train.num_epochs = 40\n",
    "        config.train.batch_size = 4\n",
    "        config.lr.init = 0.01\n",
    "        config.model.supervise_behv = False\n",
    "        config.seed = seed\n",
    "        config.model.dim_y = y.shape[2]    \n",
    "        config.model.dim_a = latent_factors     #manifold latent factors\n",
    "        config.model.dim_x = latent_factors     #dynamic latent factors (should be same as dim a)\n",
    "        config.loss.scale_l2 = scale_l2\n",
    "        config.model.save_dir = f'./results/neural/subj_{subj_num}_l2_{scale_l2}_nlatent_{latent_factors}'\n",
    "        \n",
    "        config.load.ckpt = 'best_loss'\n",
    "        trainer = TrainerDFINE(config=config)\n",
    "        \n",
    "        predictions = {\n",
    "            \"x_pred\": [],\n",
    "            \"x_filter\": [],\n",
    "            \"x_smooth\": [],\n",
    "            \"a_hat\": [],\n",
    "            \"a_pred\": [],\n",
    "            \"a_filter\": [],\n",
    "            \"a_smooth\": [],\n",
    "        }\n",
    "        \n",
    "        g_file_path = os.path.join(config.model.save_dir, 'g_latents.pt')\n",
    "        ng_file_path = os.path.join(config.model.save_dir, 'ng_latents.pt')\n",
    "        all_file_path = os.path.join(config.model.save_dir, 'batchwise_latents.pt')\n",
    "        if os.path.exists(g_file_path) and  os.path.exists(ng_file_path) and os.path.exists(all_file_path):\n",
    "            gamble_latents = torch.load(g_file_path)\n",
    "            no_gamble_latents = torch.load(ng_file_path)\n",
    "            all_latents = torch.load(all_file_path)\n",
    "            print(\"File loaded successfully.\")\n",
    "        else:\n",
    "            print(\"File does not exist.\")\n",
    "            gamble_dataset,  _,gamble_loader, _ = train_test_split(gamble, 1, batch_size = 1)\n",
    "            no_gamble_dataset,_, no_gamble_loader ,_ = train_test_split(no_gamble, 1, batch_size = 1)\n",
    "    \n",
    "            g_train_loader = DataLoader(gamble_dataset, batch_size =  1, shuffle= False)\n",
    "            gamble_latents = trainer.compute_latents(train_loader=gamble_loader)\n",
    "    \n",
    "            no_train_loader = DataLoader(no_gamble_dataset, batch_size =  1, shuffle = False) \n",
    "            no_gamble_latents = trainer.compute_latents(train_loader= no_gamble_loader)\n",
    "    \n",
    "            train_y_zsc, mean_y, std_y = z_score_tensor(y, fit=True)\n",
    "            train_dataset = DFINEDataset(y=train_y_zsc)\n",
    "            train_loader = DataLoader(train_dataset, batch_size = 1, shuffle=False)\n",
    "            all_latents = trainer.compute_latents(train_loader= train_loader)\n",
    "            torch.save(gamble_latents, g_file_path)\n",
    "            torch.save(no_gamble_latents, ng_file_path)\n",
    "            torch.save(all_latents, all_file_path)\n",
    "    \n",
    "        save_dir = os.path.join(config.model.save_dir, 'confusion_matrices')\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    \n",
    "        confusion_matrices = {}\n",
    "        num_g_trials = np.sum(labelsset)\n",
    "        num_ng_trials = y.shape[0]-num_g_trials\n",
    "        for key in gamble_latents['train'].keys():\n",
    "            if key == 'mask':\n",
    "                continue\n",
    "            total_g = torch.stack(gamble_latents['train'][key], dim=0).squeeze().sum(dim=0, keepdim = True).squeeze()[2000:3001]\n",
    "            total_ng = torch.stack(no_gamble_latents['train'][key], dim=0).squeeze().sum(dim=0, keepdim = True).squeeze()[2000:3001]\n",
    "            print(key)\n",
    "            print(\"g avg dim are: \" , total_g.shape) #(num_seq, num_steps, dim_y)\n",
    "            print(\"ng avgdim are: \" , total_ng.shape) #(num_seq, num_steps, dim_y)\n",
    "    \n",
    "            avg_latent_factor_plot(total_g/num_g_trials, config.model.save_dir, feat_name=key)\n",
    "            avg_latent_factor_plot(total_ng/num_ng_trials, config.model.save_dir, prefix=\"no gamble\", feat_name=key)\n",
    "            for trial in range(0,y.shape[0]):\n",
    "                if labelsset[trial]:\n",
    "                    gamble_avg = ((total_g - all_latents['train'][key][trial].squeeze()[2000:3001]) / (num_g_trials-1))\n",
    "                    no_gamble_avg = total_ng/(num_ng_trials)\n",
    "                    \n",
    "                else:\n",
    "                    gamble_avg = total_g/(num_g_trials)\n",
    "                    no_gamble_avg = ((total_ng - all_latents['train'][key][trial].squeeze()[2000:3001]) /( num_ng_trials-1))\n",
    "    \n",
    "                test_data = all_latents['train'][key][trial].squeeze()[2000:3001]\n",
    "    \n",
    "                g_distances = findEuclidianDist(gamble_avg, test_data)\n",
    "                ng_distances =  findEuclidianDist(no_gamble_avg, test_data)\n",
    "    \n",
    "                predictions[key].append(1 if ng_distances > g_distances else 0)\n",
    "    \n",
    "            cm = confusion_matrix(labelsset, predictions[key])\n",
    "            torch.save(cm, os.path.join(save_dir, f\"{key}_confusion_matrix.pt\"))\n",
    "    \n",
    "            disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"No Gamble\", \"Gamble\"])\n",
    "            disp.plot(cmap=\"Blues\")\n",
    "            plt.title(f\"Confusion Matrix for {key}\")\n",
    "            plt.savefig(os.path.join(save_dir, f\"{key}_confusion_matrix.png\"))\n",
    "            plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
