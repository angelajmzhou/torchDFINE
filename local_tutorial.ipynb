{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IMaaAOoFOxOL"
   },
   "source": [
    "# DFINE Tutorial\n",
    "## Overview\n",
    "\n",
    "DFINE, which stands for **Dynamical Flexible Inference for Nonlinear Embeddings**, is a neural network model that is developed to enable flexible inference, whether causally, non-causally, or even in the presence of missing neural observations. To enable flexible inference, a model must achieve all the following operations simultaneously, without the need to retrain a new model or change the inference structure:\n",
    "\n",
    "1\\) Causal inference (filtering) <br>\n",
    "2\\) Non-causal inference (smoothing) <br>\n",
    "3\\) Account for missing observations, which can occur in wireless neural interfaces\n",
    "\n",
    "DFINE achieves flexible inference. Also, DFINE’s inference is recursive and thus computationally efficient. Flexible inference is essential for developing neurotechnology, such as brain-machine interfaces (BMIs).\n",
    "\n",
    "### Model Architecture\n",
    "To achieve flexible inference, DFINE separates the model into jointly trained manifold and dynamic latent factors such that nonlinearity is captured through the manifold factors and the dynamics can be modeled in tractable linear form on this nonlinear manifold. Also, as its training loss, DFINE can use the future-step-ahead neural prediction error because of its flexible inference capability that allows it to efficiently and recursively compute this loss during training.\n",
    "\n",
    "Specifically, we define the two sets of latent factors as follows: 1) Manifold latent factors ${a}_t \\in \\mathbb{R}^{n_a \\times 1}$ and 2) Dynamic latent factors ${x}_t \\in \\mathbb{R}^{n_x \\times 1}$.\n",
    "\n",
    "First, the dynamic latent factors evolve in time with a linear Gaussian model: $\\begin{equation}{x}_{t+1} = A{x}_t + {w}_t\\tag{1}\\end{equation}$ where $A \\in \\mathbb{R}^{n_x \\times n_x}$ is the state transition matrix and ${w}_t \\in \\mathbb{R}^{n_x \\times 1}$ is zero-mean Gaussian noise with covariance matrix $W \\in \\mathbb{R}^{n_x \\times n_x}$. The manifold latent factors ${a}_t$ are related to the dynamic latent factors ${x}_t$ as: $\\begin{equation}{a}_t = C{x}_t + {r}_t\\tag{2}\\end{equation}$ where $C \\in \\mathbb{R}^{n_a \\times n_x}$ is the emission matrix and ${r}_t \\times \\mathbb{R}^{n_a \\times 1}$ is white Gaussian noise with covariance matrix $R \\in \\mathbb{R}^{n_a \\times n_a}$. Equations (1) and (2) form an LDM with learnable parameters $\\psi = \\{ A, C, W, R, {\\mu}_0, \\Lambda_0 \\}$ where ${\\mu}_0$ and $\\Lambda_0$ are the initial estimate and covariance of dynamic latent factors, respectively.\n",
    "\n",
    "Second, to model nonlinear mappings, we used MLP autoencoders to learn the mapping between neural observations ${y}_t$ and manifold latent factors ${a}_t$. We model the decoder part as a nonlinear mapping $f_\\theta(\\cdot)$ from manifold latent factors to neural observations: $\\begin{equation}{y}_t = f_\\theta({a}_t) + {v}_t\\tag{3}\\end{equation}$ where $\\theta$ are parameters and ${v}_t \\in \\mathbb{R}^{n_y \\times 1}$ is a white Gaussian noise with covariance $V \\in \\mathbb{R}^{n_y \\times n_y}$. Equations (1)-(3) together form the generative model.\n",
    "\n",
    "For inference, we also need the mapping from ${y}_t$ to ${a}_t$, which we characterize as: $\\begin{equation}{a}_t = f_\\phi ({y}_t)\\tag{4}\\end{equation}$ where $f_\\phi(\\cdot)$ represents the encoder in the autoencoder structure and is parameterized by another MLP. All equations above are trained together end-to-end, rather than separately. Further, the middle manifold layer in equation (2) explicitly incorporates a\n",
    "stochastic noise variable $r_t$, whose covariance is learned during training, allowing the nonlinearity with respect to the dynamic latent factors to be stochastic in DFINE. To help with robustness to noise and stochasticity during inference, DFINE learns all the stochastic noise distribution parameters during training, which are then explicitly accounted for at inference.\n",
    "\n",
    "\n",
    "### The Inference Problem\n",
    "Using the equations above, we can infer both the manifold and dynamic latent factors from neural observations ${y}_{1:T}$, where $T$ is the total number of time steps for the observations. We use subscript $t|k$ to denote the inferred latent factors at time $t$ given observations up to time $k$, ${y}_{1:k}$. Thus, $t|t$ denotes filtering (causal) inference given ${y}_{1:t}$, $t+k|t$ denotes the $k$-step-ahead prediction given $y_{1:t}$, and $t|T$ denotes smoothing (non-causal) inference given ${y}_{1:T}$.\n",
    "\n",
    "The inference method is shown in Figure 1b in the paper and is as follows. We first directly but statically obtain an initial estimate of ${a}_t$ based on ${y}_t$ with ${\\hat{a}}_t = f_\\phi({y}_t)$ in equation (4), to provide the noisy observations of the dynamical model, that is, ${\\hat{a}}_t$. Having obtained ${\\hat{a}}_t$, we can now use the dynamical part of the model in equations (1) and (2) to infer ${x}_{t|t}$ with Kalman filtering from ${\\hat{a}}_{1:t}$, and infer ${x}_{t|T}$ with Kalman smoothing from ${\\hat{a}}_{1∶T}$. We can then infer the manifold latent factor as ${a}_{t|t} = C{x}_{t|t}$ and ${a}_{t|T} = C{x}_{t|T}$ on the basis of equation (2). Similarly, we can obtain the filtered neural activity ${y}_{t|t}$ and smoothed neural activity ${y}_{t|T}$ using equation (3) as ${y}_{t|t} = f_{\\theta}({a}_{t|t})$ and ${y}_{t|T} = f_{\\theta}({a}_{t|T})$, respectively.\n",
    "\n",
    "To obtain the $k$-step-ahead predicted neural activity ${y}_{t+k|t}$, we first recursively forward predict the dynamic latent factors $k$ time-steps with the Kalman predictor, and obtain ${x}_{t+k|t}$. Then, we can compute the $k$-step-ahead predictions of manifold latent factors and neural observations with ${a}_{t+k|t} = C{x}_{t+k|t}$ and ${y}_{t+k|t} = f_{\\theta}({a}_{t+k|t})$, respectively.\n",
    "\n",
    "### Training Loss Function\n",
    "Having established the DFINE model and its inference, we can learn the model parameters $\\psi, \\theta, \\phi$ by minimizing:  $\\begin{equation}L(\\psi, \\theta, \\phi) = \\sum_{k=1}^K \\sum_{t=1}^{T-k} e({y}_{t+k|t}, {y}_{t+k}) + \\lambda_{reg} L_2 (\\theta, \\phi)\\tag{5}\\end{equation}$ where $K$ denotes the maximum horizon for future-step-ahead prediction, $e(\\cdot, \\cdot)$ denotes the error measure which is taken as mean-squared error (MSE) loss, $L_2(\\cdot)$ is L2 regularization for the autoencoder parameters $\\{\\theta, \\phi\\}$ to prevent overfitting, and $\\lambda_{reg}$ is the L2 regularization loss scale (see config_dfine.py).\n",
    "\n",
    "### Training tips and hyperparameters\n",
    "DFINE does not have many hyperparameters to tune. Yet **it may be necessary to search over a grid of the following hyperparameters to find the best performing ones for a given dataset (especially for L2 regularization loss scale)**:\n",
    "- L2 regularization loss scale, config.loss.scale_l2. For hyperparameter search, you can use a small grid such as [1e-4, 5e-4, 1e-3, 2e-3] after z-scoring the signals (see below).\n",
    "- $K$, future-step-ahead prediction horizon provided as a list, config.loss.steps_ahead\n",
    "- Encoder/decoder architecture, i.e., number of hidden layers and units in each layer, config.model.hidden_layer_list\n",
    "- Setting $n_a$ higher than $n_y$ may lead to overfitting, it's recommended that $n_a \\leq n_y$\n",
    "- As we show in Extended Data Fig. 8, it's recommended to set $n_a = n_x$ to reduce the hyperparameter search complexity\n",
    "\n",
    "For default values of all hyperparameters, please see config_dfine.py. For the future-step-ahead prediction horizon, we used $K=4$, or config.loss.steps_ahead = [1,2,3,4] for DFINE.\n",
    "\n",
    "It is important to note that for neural signals, we performed **z-scoring** which is highly recommended, please see below and time_series_utils.z_score_tensor. Recommendations above for L2 regularization scale are with z-scoring, which can affect the choice of L2 regularization scale.\n",
    "\n",
    "\n",
    "**DFINE is currently implemented for continuous-valued signals**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\envs\\DFINE_env\\lib\\site-packages\\torchaudio\\backend\\utils.py:66: UserWarning: No audio backend is available.\n",
      "  warnings.warn(\"No audio backend is available.\")\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "from config_dfine import get_default_config\n",
    "from trainers.TrainerDFINE import TrainerDFINE\n",
    "from datasets import DFINEDataset\n",
    "from time_series_utils import z_score_tensor, get_nrmse_error\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def set_seed(seed=0):\n",
    "    torch.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "def train_test_split(y, train_ratio, batch_size = 32, shuffleTrain = True):\n",
    "# Split data into training and test datasets\n",
    "    num_trials = y.shape[0]\n",
    "    num_train_trials = int(train_ratio * num_trials)\n",
    "    num_test_trials = num_trials - num_train_trials\n",
    "    train_y = y[:num_train_trials, ...]\n",
    "    test_y = y[num_train_trials:, ...]\n",
    "\n",
    "    # Z-score the observation tensors\n",
    "    train_y_zsc, mean_y, std_y = z_score_tensor(train_y, fit=True)\n",
    "    test_y_zsc, _, _ = z_score_tensor(test_y, mean=mean_y, std=std_y, fit=False)\n",
    "\n",
    "    # Create DFINE dataset objects and torch dataloaders\n",
    "    train_dataset = DFINEDataset(y=train_y_zsc)\n",
    "    test_dataset = DFINEDataset(y=test_y_zsc)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size = batch_size, shuffle= shuffleTrain)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    return train_dataset, test_dataset, train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for subj in range(12, 21):\n",
    "    subj_num = f\"{subj:02}\" \n",
    "\n",
    "    f = h5py.File('GTH_data\\GTH_s' + str(subj_num) + '_decision_power_struct_nobs.mat', 'r')\n",
    "    dataset = list(f[\"power_struct\"][\"highgamma\"][\"powspctrm\"])\n",
    "    labelsset = np.array(f[\"power_struct\"][\"beh\"][\"gambles\"])\n",
    "    \n",
    "    gen_data = f[\"power_struct\"][\"highgamma\"][\"powspctrm\"]\n",
    "    highgamma = np.array(gen_data) # (time, electrodes, trials)\n",
    "    y= np.moveaxis(highgamma,2,0) # (trials, time, channels)\n",
    "    print(\"dim are: \" , y.shape) #(num_seq, num_steps, dim_y)\n",
    "    seed=0\n",
    "    set_seed(seed)\n",
    "    config = get_default_config()\n",
    "    config.device = 'cuda'\n",
    "    config.model.activation ='tanh'\n",
    "    config.train.num_epochs = 200\n",
    "    config.model.supervise_behv = False\n",
    "    config.seed = seed\n",
    "    config.model.dim_y = y.shape[2]    \n",
    "    config.model.dim_a = 3     #manifold latent factors\n",
    "    config.model.dim_x = 3     #dynamic latent factors (should be same as dim a)\n",
    "    config.loss.scale_l2 = 0\n",
    "    config.model.save_dir = f'./results/neural/subj_{subj_num}'\n",
    "\n",
    "    trainer = TrainerDFINE(config=config)\n",
    "\n",
    "\n",
    "    labelsset = labelsset.T\n",
    "    behv_mask = torch.tensor(labelsset)\n",
    "    behv_mask = behv_mask.squeeze() \n",
    "\n",
    "    # Filter trials based on the mask\n",
    "    gamble = y[behv_mask == 1]  # Trials where the mask is 1\n",
    "    no_gamble = y[behv_mask == 0]  # Trials where the mask is 0\n",
    "\n",
    "    # Output shapes\n",
    "    print(\"Gamble shape:\", gamble.shape)  # (num_seq_1, 5000, 32)\n",
    "    print(\"No Gamble shape:\", no_gamble.shape)  # (num_seq_0, 5000, 32)\n",
    "\n",
    "    train_dataset, test_dataset, train_loader, test_loader = train_test_split(y, 0.8)\n",
    "\n",
    "    trainer_load = TrainerDFINE(config=config)\n",
    "    \n",
    "    trainer_load.train(train_loader=train_loader,\n",
    "             valid_loader=test_loader)\n",
    "\n",
    "    print(\"train over\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "def findEuclidianDist(series1, series2):\n",
    "\n",
    "    # Compute Euclidean distances for each time step\n",
    "    distances = torch.sqrt(torch.sum((series1 - series2) ** 2, dim=1))  #sum over dimension tuples\n",
    "\n",
    "    # Overall distance (optional)\n",
    "    total_distance = distances.sum()  # Total distance\n",
    "    mean_distance = distances.mean()  # Total distance\n",
    "\n",
    "    return total_distance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dim are:  (180, 5001, 57)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/09/2025 05:26:51 PM - DFINE Logger - WARNING - Optimizer and LR scheduler can be loaded only in resume_train mode, else they are re-initialized\n",
      "01/09/2025 05:26:51 PM - DFINE Logger - INFO - Loading model from: ./results/neural/subj_01\\ckpts\\best_loss_ckpt.pth...\n",
      "01/09/2025 05:26:51 PM - DFINE Logger - INFO - Checkpoint succesfully loaded from ./results/neural/subj_01\\ckpts\\best_loss_ckpt.pth!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainerDFINE loaded from: C:\\Users\\angel\\Desktop\\torchDFINE\\trainers\\TrainerDFINE.py\n",
      "File loaded successfully.\n",
      "x_pred\n",
      "x_filter\n",
      "x_smooth\n",
      "a_hat\n",
      "a_pred\n",
      "a_filter\n",
      "a_smooth\n",
      "mask\n",
      "dim are:  (188, 5001, 60)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/09/2025 05:27:03 PM - DFINE Logger - WARNING - Optimizer and LR scheduler can be loaded only in resume_train mode, else they are re-initialized\n",
      "01/09/2025 05:27:03 PM - DFINE Logger - INFO - Loading model from: ./results/neural/subj_02\\ckpts\\best_loss_ckpt.pth...\n",
      "01/09/2025 05:27:03 PM - DFINE Logger - INFO - Checkpoint succesfully loaded from ./results/neural/subj_02\\ckpts\\best_loss_ckpt.pth!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainerDFINE loaded from: C:\\Users\\angel\\Desktop\\torchDFINE\\trainers\\TrainerDFINE.py\n",
      "File loaded successfully.\n",
      "x_pred\n",
      "x_filter\n",
      "x_smooth\n",
      "a_hat\n",
      "a_pred\n",
      "a_filter\n",
      "a_smooth\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "for subj in range(1, 21):\n",
    "\n",
    "    subj_num = f\"{subj:02}\" \n",
    "    \n",
    "    f = h5py.File('GTH_data\\GTH_s' + str(subj_num) + '_decision_power_struct_nobs.mat', 'r')\n",
    "    dataset = list(f[\"power_struct\"][\"highgamma\"][\"powspctrm\"])\n",
    "    labelsset = np.array(f[\"power_struct\"][\"beh\"][\"gambles\"])\n",
    "    \n",
    "    gen_data = f[\"power_struct\"][\"highgamma\"][\"powspctrm\"]\n",
    "    highgamma = np.array(gen_data) # (time, electrodes, trials)\n",
    "    y= np.moveaxis(highgamma,2,0) # (trials, time, channels)\n",
    "    print(\"dim are: \" , y.shape) #(num_seq, num_steps, dim_y)config = get_default_config()\n",
    "    \n",
    "    labelsset = labelsset.T\n",
    "    behv_mask = torch.tensor(labelsset)\n",
    "    behv_mask = behv_mask.squeeze() \n",
    "\n",
    "    gamble = y[behv_mask == 1][:][2000:3001]  \n",
    "    no_gamble = y[behv_mask == 0][:][2000:3001]  \n",
    "\n",
    "    seed = 0\n",
    "    set_seed(seed)\n",
    "    config = get_default_config()\n",
    "    config.device = 'cuda'\n",
    "    config.model.activation ='tanh'\n",
    "    config.train.num_epochs = 200\n",
    "    config.model.supervise_behv = False\n",
    "    config.seed = seed\n",
    "    config.model.dim_y = y.shape[2]    \n",
    "    config.model.dim_a = 3     #manifold latent factors\n",
    "    config.model.dim_x = 3     #dynamic latent factors (should be same as dim a)\n",
    "    config.loss.scale_l2 = 0\n",
    "    config.model.save_dir = f'./results/neural/subj_{subj_num}'\n",
    "    \n",
    "    config.load.ckpt = 'best_loss'\n",
    "    trainer = TrainerDFINE(config=config)\n",
    "    \n",
    "    predictions = {\n",
    "        \"x_pred\": [],\n",
    "        \"x_filter\": [],\n",
    "        \"x_smooth\": [],\n",
    "        \"a_hat\": [],\n",
    "        \"a_pred\": [],\n",
    "        \"a_filter\": [],\n",
    "        \"a_smooth\": [],\n",
    "    }\n",
    "    \n",
    "    g_file_path = os.path.join(config.model.save_dir, 'g_latents.pt')\n",
    "    ng_file_path = os.path.join(config.model.save_dir, 'ng_latents.pt')\n",
    "    all_file_path = os.path.join(config.model.save_dir, 'batchwise_latents.pt')\n",
    "    if os.path.exists(g_file_path) and  os.path.exists(ng_file_path) and os.path.exists(all_file_path):\n",
    "        gamble_latents = torch.load(g_file_path)\n",
    "        no_gamble_latents = torch.load(ng_file_path)\n",
    "        all_latents = torch.load(all_file_path)\n",
    "        print(\"File loaded successfully.\")\n",
    "    else:\n",
    "        print(\"File does not exist.\")\n",
    "        gamble_dataset,  _,gamble_loader, _ = train_test_split(gamble, 1, batch_size = 1)\n",
    "        no_gamble_dataset,_, no_gamble_loader ,_ = train_test_split(no_gamble, 1, batch_size = 1)\n",
    "\n",
    "        g_train_loader = DataLoader(gamble_dataset, batch_size =  1, shuffle= False)\n",
    "        gamble_latents = trainer.compute_latents(train_loader=gamble_loader)\n",
    "\n",
    "        no_train_loader = DataLoader(no_gamble_dataset, batch_size =  1, shuffle = False) \n",
    "        no_gamble_latents = trainer.compute_latents(train_loader= no_gamble_loader)\n",
    "\n",
    "        train_y_zsc, mean_y, std_y = z_score_tensor(y, fit=True)\n",
    "        train_dataset = DFINEDataset(y=train_y_zsc)\n",
    "        train_loader = DataLoader(train_dataset, batch_size = 1, shuffle=False)\n",
    "        all_latents = trainer.compute_latents(train_loader= train_loader)\n",
    "        torch.save(gamble_latents, g_file_path)\n",
    "        torch.save(no_gamble_latents, ng_file_path)\n",
    "        torch.save(all_latents, all_file_path)\n",
    "\n",
    "    save_dir = os.path.join(config.model.save_dir, 'confusion_matrices')\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "    confusion_matrices = {}\n",
    "    num_trials = y.shape[0]-1\n",
    "    \n",
    "    for key in gamble_latents['train'].keys():\n",
    "        total_g = torch.stack(gamble_latents['train'][key], dim=0).sum(dim=0, keepdim = True)\n",
    "        total_ng = torch.stack(no_gamble_latents['train'][key], dim=0).sum(dim=0, keepdim = True)\n",
    "        print(key)\n",
    "        if key == 'mask':\n",
    "            continue\n",
    "        for trial in range(0,y.shape[0]):\n",
    "            if labelsset[trial]:\n",
    "                gamble_avg = ((total_g - all_latents['train'][key][trial]) / num_trials).squeeze()\n",
    "                no_gamble_avg = torch.mean(total_ng, dim=0).squeeze()\n",
    "            else:\n",
    "                gamble_avg = torch.mean(total_g, dim=0).squeeze()\n",
    "                no_gamble_avg = ((total_ng - all_latents['train'][key][trial]) / num_trials).squeeze()\n",
    "\n",
    "            test_data = all_latents['train'][key][trial].squeeze()\n",
    "            # print(\"gamble dim are: \" , gamble_avg.shape) #(num_seq, num_steps, dim_y)config = get_default_config()\n",
    "            # print(\"ng dim are: \" , no_gamble_avg.shape) #(num_seq, num_steps, dim_y)config = get_default_config()\n",
    "\n",
    "            g_distances = findEuclidianDist(gamble_avg, test_data)\n",
    "            ng_distances =  findEuclidianDist(no_gamble_avg, test_data)\n",
    "\n",
    "            # Predictions\n",
    "            predictions[key].append(1 if ng_distances > g_distances else 0)\n",
    "\n",
    "        cm = confusion_matrix(labelsset, predictions[key])\n",
    "        torch.save(cm, os.path.join(save_dir, f\"{key}_confusion_matrix.pt\"))\n",
    "\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"No Gamble\", \"Gamble\"])\n",
    "        disp.plot(cmap=\"Blues\")\n",
    "        plt.title(f\"Confusion Matrix for {key}\")\n",
    "        plt.savefig(os.path.join(save_dir, f\"{key}_confusion_matrix.png\"))\n",
    "        plt.close()\n",
    "\n",
    "    #save as .pt file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import h5py\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for subj in range(1, 21):\n",
    "    \n",
    "    subj_num = f\"{subj:02}\" \n",
    "    \n",
    "    f = h5py.File('GTH_data\\GTH_s' + str(subj_num) + '_decision_power_struct_nobs.mat', 'r')\n",
    "    dataset = list(f[\"power_struct\"][\"highgamma\"][\"powspctrm\"])\n",
    "    labelsset = np.array(f[\"power_struct\"][\"beh\"][\"gambles\"])\n",
    "    \n",
    "    gen_data = f[\"power_struct\"][\"highgamma\"][\"powspctrm\"]\n",
    "    highgamma = np.array(gen_data) # (time, electrodes, trials)\n",
    "    y= np.moveaxis(highgamma,2,0) # (trials, time, channels)\n",
    "    print(\"dim are: \" , y.shape) #(num_seq, num_steps, dim_y)config = get_default_config()\n",
    "\n",
    "    seed = 0\n",
    "    set_seed(seed)\n",
    "    config = get_default_config()\n",
    "    config.device = 'cuda'\n",
    "    config.model.activation ='tanh'\n",
    "    config.train.num_epochs = 200\n",
    "    config.model.supervise_behv = False\n",
    "    config.seed = seed\n",
    "    config.model.dim_y = y.shape[2]    \n",
    "    config.model.dim_a = 3     #manifold latent factors\n",
    "    config.model.dim_x = 3     #dynamic latent factors (should be same as dim a)\n",
    "    config.loss.scale_l2 = 0\n",
    "    config.model.save_dir = f'./results/neural/subj_{subj_num}'\n",
    "    \n",
    "    config.load.ckpt = 'best_loss'\n",
    "    trainer_load = TrainerDFINE(config=config)\n",
    "    _load_ckpt(trainer_load, torch.optim.Adam, lr_scheduler=None)\n",
    "    \n",
    "    predictions = {\n",
    "        \"x_pred\": [],\n",
    "        \"x_filter\": [],\n",
    "        \"x_smooth\": [],\n",
    "        \"a_hat\": [],\n",
    "        \"a_pred\": [],\n",
    "        \"a_filter\": [],\n",
    "        \"a_smooth\": [],\n",
    "    }\n",
    "    true = labelsset\n",
    "    \n",
    "    \n",
    "    gamble_dataset,  _,gamble_loader, _ = train_test_split(gamble, 1, batch_size = 1)\n",
    "    no_gamble_dataset,_, no_gamble_loader ,_ = train_test_split(no_gamble, 1, btch_size = 1)\n",
    "\n",
    "    g_train_loader = DataLoader(gamble_dataset, batch_size =  1, shuffle=False) # make shuffle False to keep track of segment ids\n",
    "    gamble_latents = trainer.compute_avg_latents(gamble = True, train_loader=gamble_loader, subjnum = subj_num)\n",
    "\n",
    "    ng_train_loader = DataLoader(no_gamble_dataset, batch_size =  1, shuffle=False) # make shuffle False to keep track of segment ids\n",
    "    ng_gamble_latents = trainer.compute_avg_latents(gamble = False, train_loader= no_gamble_loader, subjnum = subj_num)\n",
    "\n",
    "    all_latents = trainer.compute_latents( train_loader= no_gamble_loader, subjnum = subj_num)\n",
    "    for trial in range(1,y.shape[0]):\n",
    "        \n",
    "        tens_y = torch.from_numpy(y[trial])\n",
    "        tens_y = torch.unsqueeze(tens_y, dim=0)\n",
    "        print(tens_y.shape)\n",
    "        tens_y = tens_y.float()\n",
    "        test_y_zsc, mean_y, std_y = z_score_tensor(tens_y, fit=True)\n",
    "\n",
    "        test_trial = DFINEDataset(y=test_y_zsc)\n",
    "        test_loader = DataLoader(test_trial, batch_size = 1, shuffle=False)\n",
    "        test_latents = trainer.calculate_latents(train_loader=test_loader)\n",
    "\n",
    "        for key in gamble_latents['train'].keys():\n",
    "            gamble_avg = gamble_latents['train'][key].squeeze()\n",
    "            no_gamble_avg = ng_gamble_latents['train'][key].squeeze()\n",
    "            test_data = test_latents['train'][key].squeeze()\n",
    "\n",
    "            print(\"gamble dist:\")\n",
    "            g = findEuclidianDist(test_data, gamble_avg)\n",
    "            print(\"no gamble dist:\")\n",
    "            ng = findEuclidianDist(test_data, no_gamble_avg)\n",
    "            \n",
    "            predictions[key].append(1 if ng>g else 0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z-IpOXIJPrMp",
    "outputId": "490146ee-a4ec-4e80-8f0d-fdd3a0b1bc89"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b6bjH4YuAVs_",
    "outputId": "43841708-328a-463b-ed59-160ec66e7a48"
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "subj_num = 13\n",
    "f = h5py.File('GTH_data\\GTH_s' + str(subj_num) + '_decision_power_struct_nobs.mat', 'r')\n",
    "dataset = list(f[\"power_struct\"][\"highgamma\"][\"powspctrm\"])\n",
    "#high gamma-- high freq, lots of data here\n",
    "#power spectrum -- the name of the data and how it's stored\n",
    "#power just indicates that it's frequency\n",
    "labelsset = np.array(f[\"power_struct\"][\"beh\"][\"gambles\"])\n",
    "print(\"labelset dim are: \", labelsset.shape)\n",
    "print(labelsset)\n",
    "#binary rep of choice to gamble\n",
    "#behavior, contains all behavior data incl. label set.\n",
    "\n",
    "#use high gamma to predict whether participant gambled: ignore everything else."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k7Rz6L6UOxOO"
   },
   "source": [
    "## Setting up the Virtual Environment\n",
    "\n",
    "First, let's make sure the virtual environment requirements are satisfied."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JmgIqEcEOxOQ"
   },
   "source": [
    "We installed/checked all required packages to run DFINE training, let's start with importing the following libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WUX0rfmFOxOR"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from config_dfine import get_default_config\n",
    "from trainers.TrainerDFINE import TrainerDFINE\n",
    "from datasets import DFINEDataset\n",
    "from time_series_utils import z_score_tensor, get_nrmse_error\n",
    "\n",
    "# Sets the seed\n",
    "seed = 0\n",
    "def set_seed(seed=0):\n",
    "    torch.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "# Plotting helper functions\n",
    "def set_matplotlib_starter_nature():\n",
    "    matplotlib.rcParams['pdf.fonttype'] = 42\n",
    "    matplotlib.rcParams['ps.fonttype'] = 42\n",
    "    font = {'family' : 'sans-serif',\n",
    "            'weight' : 'normal',\n",
    "            'size'   : 15}\n",
    "    matplotlib.rc('font', **font)\n",
    "\n",
    "def set_plot_settings_nature(fig,ax_list):\n",
    "    for ax in ax_list:\n",
    "        ax.spines[\"top\"].set_visible(False)\n",
    "        ax.spines[\"right\"].set_visible(False)\n",
    "        ax.spines[\"bottom\"].set_color('black')\n",
    "        ax.spines[\"left\"].set_color('black')\n",
    "        ax.tick_params('both', length=8, width=1, which='major', color='black')\n",
    "        ax.tick_params('both', length=4, width=0.5, which='minor', color='black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ur1MNOVhOxOT"
   },
   "source": [
    "## DFINE Training\n",
    "\n",
    "At this point, the data is loaded. Now, let's get the default DFINE config, and change the dimensionality related hyperparameters, result saving directory and other parameters if desired (see config_dfine.py for explanations and default values). As shown in the paper, Extended Data Fig. 8, $n_a = n_x$ is a reasonable choice and reduces the computational complexity for dimension search (config.model.dim_x and config.model.dim_a) and for the Swiss-roll simulation we set them to dimensionality of $d_t$, which is 2. Please also see other \"Training tips and hyperparameters\" mentioned at the beginning of the tutorial.\n",
    "\n",
    "**L2 regularization loss scale is dependent on the observation data scale, and steps_ahead as they affect the model loss significantly. Therefore, we highly recommend a small grid search for these hyperparameters.**\n",
    "\n",
    "**For the nonlinear manifold simulations, 4 layers of 20 units was used. However, for the Lorenz simulations and data analyses in the paper, we used 3 layers of 32 units.**\n",
    "\n",
    "With the generated config, we initialize the TrainerDFINE object, which takes care of model training and inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(y, train_ratio, batch_size = 32):\n",
    "# Split data into training and test datasets\n",
    "    num_trials = y.shape[0]\n",
    "    num_train_trials = int(train_ratio * num_trials)\n",
    "    num_test_trials = num_trials - num_train_trials\n",
    "    train_y = y[:num_train_trials, ...]\n",
    "    test_y = y[num_train_trials:, ...]\n",
    "\n",
    "    # Z-score the observation tensors\n",
    "    train_y_zsc, mean_y, std_y = z_score_tensor(train_y, fit=True)\n",
    "    test_y_zsc, _, _ = z_score_tensor(test_y, mean=mean_y, std=std_y, fit=False)\n",
    "\n",
    "    # Create DFINE dataset objects and torch dataloaders\n",
    "    train_dataset = DFINEDataset(y=train_y_zsc)\n",
    "    test_dataset = DFINEDataset(y=test_y_zsc)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size = batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    return train_dataset, test_dataset, train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the seed for reproducibility\n",
    "set_seed(seed)\n",
    "\n",
    "# Get default DFINE config\n",
    "config = get_default_config()\n",
    "\n",
    "\n",
    "# Change desired hyperparameters (for nonlinear manifold simulations)\n",
    "#need to change parameters\n",
    "\n",
    "#Set the device to 'cuda' if you have a GPU available for faster training.\n",
    "config.device = 'cuda'\n",
    "\n",
    "config.model.activation ='tanh'\n",
    "config.train.num_epochs = 200\n",
    "\n",
    "config.model.supervise_behv = False\n",
    "\n",
    "config.seed = seed\n",
    "config.model.dim_y = 32    \n",
    "config.model.dim_a = 3     #manifold latent factors\n",
    "config.model.dim_x = 3     #dynamic latent factors (should be same as dim a)\n",
    "config.loss.scale_l2 = 0\n",
    "config.model.save_dir = f'./results/neural'\n",
    "\n",
    "# Initialize DFINE Trainer\n",
    "trainer = TrainerDFINE(config=config)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_data = f[\"power_struct\"][\"highgamma\"][\"powspctrm\"]\n",
    "highgamma = np.array(gen_data) # (time, electrodes, trials)\n",
    "y= np.moveaxis(highgamma,2,0) # (trials, time, channels)\n",
    "print(\"dim are: \" , y.shape) #(num_seq, num_steps, dim_y)\n",
    "\n",
    "# 5000ms length behavioral variable, load in length and trial.\n",
    "\n",
    "#5000 is time (5 sec data, 1000 hz)\n",
    "#32: # electrodes\n",
    "#200: # trials\n",
    "#vary subject by subject\n",
    "\n",
    "#least first trial out for classification later\n",
    "test_y = y[0]\n",
    "y = y[1:]\n",
    "\n",
    "labelsset = labelsset.T\n",
    "behv_mask = torch.tensor(labelsset)\n",
    "behv_mask = behv_mask.squeeze() \n",
    "\n",
    "test_y_type = behv_mask[0]\n",
    "behv_mask = behv_mask[1:]\n",
    "\n",
    "# Filter trials based on the mask\n",
    "gamble = y[behv_mask == 1]  # Trials where the mask is 1\n",
    "no_gamble = y[behv_mask == 0]  # Trials where the mask is 0\n",
    "\n",
    "# Output shapes\n",
    "print(\"Gamble shape:\", gamble.shape)  # (num_seq_1, 5000, 32)\n",
    "print(\"No Gamble shape:\", no_gamble.shape)  # (num_seq_0, 5000, 32)\n",
    "\n",
    "train_dataset, test_dataset, train_loader, test_loader = train_test_split(y, 0.8)\n",
    "\n",
    "gamble_dataset,  _,gamble_loader, _ = train_test_split(gamble, 1, batch_size = 1)\n",
    "no_gamble_dataset,_, no_gamble_loader ,_ = train_test_split(no_gamble, 1,batch_size = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for subj in range(11, 12):\n",
    "    \n",
    "    subj_num = f\"{subj:02}\" \n",
    "    \n",
    "    f = h5py.File('GTH_data\\GTH_s' + str(subj_num) + '_decision_power_struct_nobs.mat', 'r')\n",
    "    dataset = list(f[\"power_struct\"][\"highgamma\"][\"powspctrm\"])\n",
    "    labelsset = np.array(f[\"power_struct\"][\"beh\"][\"gambles\"])\n",
    "    \n",
    "    gen_data = f[\"power_struct\"][\"highgamma\"][\"powspctrm\"]\n",
    "    highgamma = np.array(gen_data) # (time, electrodes, trials)\n",
    "    y= np.moveaxis(highgamma,2,0) # (trials, time, channels)\n",
    "    print(\"dim are: \" , y.shape) #(num_seq, num_steps, dim_y)config = get_default_config()\n",
    "    seed = 0\n",
    "    set_seed(seed)\n",
    "    config = get_default_config()\n",
    "    config.device = 'cuda'\n",
    "    config.model.activation ='tanh'\n",
    "    config.train.num_epochs = 200\n",
    "    config.model.supervise_behv = False\n",
    "    config.seed = seed\n",
    "    config.model.dim_y = y.shape[2]    \n",
    "    config.model.dim_a = 3     #manifold latent factors\n",
    "    config.model.dim_x = 3     #dynamic latent factors (should be same as dim a)\n",
    "    config.loss.scale_l2 = 0\n",
    "    config.model.save_dir = f'./results/neural/subj_{subj_num}'\n",
    "    config.load.resume_train = True\n",
    "    config.load.ckpt = '10'\n",
    "    train_dataset, test_dataset, train_loader, test_loader = train_test_split(y, 0.8)\n",
    "\n",
    "    trainer_load = TrainerDFINE(config=config)\n",
    "    \n",
    "    trainer_load.train(train_loader=train_loader,\n",
    "             valid_loader=test_loader)\n",
    "\n",
    "    print(\"train over\")\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ftV78Z5OxOT"
   },
   "source": [
    "The last step before model training is training and test dataset/dataloader generation. We use 80% of the trials for training, and the remaining 20% for test. If someone desires a hyperparameter search, an inner cross-validation can be performed by splitting the training dataset into training and validation\n",
    "datasets, where hyperparameters are tuned based on performance on the validation dataset (such as one-step-ahead prediction MSE, correlation coefficient, etc.).\n",
    "\n",
    "DFINE can also be trained and do inference on a continuous stream of data. If desired, the generated trials can be concatenated to form a single, long continuous data stream and DFINE can be trained on that and inference can be performed on that as well.\n",
    "\n",
    "As noted above, for all analyses in the paper, we performed **z-scoring** across time on neural signals whose statistics are obtained on the training set. We follow the same structure here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uLf7mSjkOxOT"
   },
   "source": [
    "Now, it's time for training. Running 'trainer.train' starts training and after\n",
    "training is complete, we call 'trainer.save_encoding_results' to perform inference on training and test datasets. If you want to save the inference results in a file named 'encoding_results.pt' under config.model.save_dir, please make save_results True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "2qYwWtj4OxOT",
    "outputId": "a04a4179-1f1d-48e1-e484-329cd3adeb92",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 182, VALID: 100%|██████████| 2/2 [00:09<00:00,  4.61s/batch]\n",
      "Epoch 183, TRAIN: 100%|██████████| 5/5 [00:52<00:00, 10.48s/batch]\n",
      "Epoch 183, VALID: 100%|██████████| 2/2 [00:09<00:00,  4.73s/batch]\n",
      "Epoch 184, TRAIN: 100%|██████████| 5/5 [00:52<00:00, 10.56s/batch]\n",
      "Epoch 184, VALID: 100%|██████████| 2/2 [00:09<00:00,  4.68s/batch]\n",
      "Epoch 185, TRAIN: 100%|██████████| 5/5 [00:51<00:00, 10.29s/batch]\n",
      "Epoch 185, VALID: 100%|██████████| 2/2 [00:09<00:00,  4.78s/batch]\n",
      "Epoch 186, TRAIN: 100%|██████████| 5/5 [00:50<00:00, 10.13s/batch]\n",
      "Epoch 186, VALID: 100%|██████████| 2/2 [00:08<00:00,  4.44s/batch]\n",
      "Epoch 187, TRAIN: 100%|██████████| 5/5 [00:50<00:00, 10.17s/batch]\n",
      "Epoch 187, VALID: 100%|██████████| 2/2 [00:08<00:00,  4.47s/batch]\n",
      "Epoch 188, TRAIN: 100%|██████████| 5/5 [00:50<00:00, 10.17s/batch]\n",
      "Epoch 188, VALID: 100%|██████████| 2/2 [00:09<00:00,  4.58s/batch]\n",
      "Epoch 189, TRAIN: 100%|██████████| 5/5 [00:51<00:00, 10.38s/batch]\n",
      "Epoch 189, VALID: 100%|██████████| 2/2 [00:09<00:00,  4.65s/batch]\n",
      "Epoch 190, TRAIN: 100%|██████████| 5/5 [00:51<00:00, 10.33s/batch]\n",
      "12/02/2024 02:23:30 PM - DFINE Logger - INFO - Epoch 190, TRAIN\n",
      "1_step_mse: 0.58980\n",
      "2_steps_mse: 0.62999\n",
      "3_steps_mse: 0.68983\n",
      "4_steps_mse: 0.75691\n",
      "reg_loss: 0.00000, scale_l2: 0.00000\n",
      "model_loss: 2.66654, total_loss: 2.66654\n",
      "\n",
      "Epoch 190, VALID: 100%|██████████| 2/2 [00:09<00:00,  4.66s/batch]\n",
      "12/02/2024 02:23:44 PM - DFINE Logger - INFO - Epoch 190, VALID\n",
      "1_step_mse: 1.12889\n",
      "2_steps_mse: 1.20727\n",
      "3_steps_mse: 1.31808\n",
      "4_steps_mse: 1.44064\n",
      "reg_loss: 0.00000, scale_l2: 0.00000\n",
      "model_loss: 5.09488, total_loss: 5.09488\n",
      "\n",
      "Epoch 191, TRAIN: 100%|██████████| 5/5 [00:52<00:00, 10.41s/batch]\n",
      "Epoch 191, VALID: 100%|██████████| 2/2 [00:09<00:00,  4.60s/batch]\n",
      "Epoch 192, TRAIN: 100%|██████████| 5/5 [00:52<00:00, 10.44s/batch]\n",
      "Epoch 192, VALID: 100%|██████████| 2/2 [00:09<00:00,  4.65s/batch]\n",
      "Epoch 193, TRAIN: 100%|██████████| 5/5 [00:52<00:00, 10.57s/batch]\n",
      "Epoch 193, VALID: 100%|██████████| 2/2 [00:09<00:00,  4.64s/batch]\n",
      "Epoch 194, TRAIN: 100%|██████████| 5/5 [00:51<00:00, 10.33s/batch]\n",
      "Epoch 194, VALID: 100%|██████████| 2/2 [00:09<00:00,  4.80s/batch]\n",
      "Epoch 195, TRAIN: 100%|██████████| 5/5 [00:51<00:00, 10.37s/batch]\n",
      "Epoch 195, VALID: 100%|██████████| 2/2 [00:09<00:00,  4.70s/batch]\n",
      "Epoch 196, TRAIN: 100%|██████████| 5/5 [00:52<00:00, 10.43s/batch]\n",
      "Epoch 196, VALID: 100%|██████████| 2/2 [00:09<00:00,  4.70s/batch]\n",
      "Epoch 197, TRAIN: 100%|██████████| 5/5 [00:52<00:00, 10.51s/batch]\n",
      "Epoch 197, VALID: 100%|██████████| 2/2 [00:09<00:00,  4.69s/batch]\n",
      "Epoch 198, TRAIN: 100%|██████████| 5/5 [00:51<00:00, 10.34s/batch]\n",
      "Epoch 198, VALID: 100%|██████████| 2/2 [00:09<00:00,  4.68s/batch]\n",
      "Epoch 199, TRAIN: 100%|██████████| 5/5 [00:51<00:00, 10.34s/batch]\n",
      "Epoch 199, VALID: 100%|██████████| 2/2 [00:09<00:00,  4.64s/batch]\n",
      "Epoch 200, TRAIN: 100%|██████████| 5/5 [00:52<00:00, 10.41s/batch]\n",
      "12/02/2024 02:34:40 PM - DFINE Logger - INFO - Epoch 200, TRAIN\n",
      "1_step_mse: 0.58691\n",
      "2_steps_mse: 0.62750\n",
      "3_steps_mse: 0.68773\n",
      "4_steps_mse: 0.75511\n",
      "reg_loss: 0.00000, scale_l2: 0.00000\n",
      "model_loss: 2.65726, total_loss: 2.65726\n",
      "\n",
      "Epoch 200, VALID: 100%|██████████| 2/2 [00:09<00:00,  4.58s/batch]\n",
      "12/02/2024 02:34:58 PM - DFINE Logger - INFO - Epoch 200, VALID\n",
      "1_step_mse: 1.12748\n",
      "2_steps_mse: 1.20458\n",
      "3_steps_mse: 1.31459\n",
      "4_steps_mse: 1.43650\n",
      "reg_loss: 0.00000, scale_l2: 0.00000\n",
      "model_loss: 5.08315, total_loss: 5.08315\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train over\n"
     ]
    }
   ],
   "source": [
    "trainer.train(train_loader=train_loader,\n",
    "             valid_loader=test_loader)\n",
    "\n",
    "print(\"train over\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To Load Previous Weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.load.resume_train = True\n",
    "config.load.ckpt = 'best_loss'\n",
    "\n",
    "trainer_load = TrainerDFINE(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_train_loader = DataLoader(gamble_dataset, batch_size =  1, shuffle=False) # make shuffle False to keep track of segment ids\n",
    "gamble_latents = trainer.compute_avg_latents(gamble = True, train_loader=gamble_loader, subjnum = subj_num)\n",
    "\n",
    "ng_train_loader = DataLoader(no_gamble_dataset, batch_size =  1, shuffle=False) # make shuffle False to keep track of segment ids\n",
    "ng_gamble_latents = trainer.compute_avg_latents(gamble = False, train_loader= no_gamble_loader, subjnum = subj_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findEuclidianDist(series1, series2):\n",
    "\n",
    "    # Compute Euclidean distances for each time step\n",
    "    distances = torch.sqrt(torch.sum((series1 - series2) ** 2, dim=1))  #sum over dimension tuples\n",
    "\n",
    "    # Overall distance (optional)\n",
    "    total_distance = distances.sum()  # Total distance\n",
    "    mean_distance = distances.mean()  # Total distance\n",
    "\n",
    "    \n",
    "    print(f\"Distances at each time step: {distances}\")\n",
    "    print(f\"Mean distance: {mean_distance}\")\n",
    "    print(f\"Total distance: {total_distance}\")\n",
    "    return total_distance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tens_y = torch.from_numpy(test_y)\n",
    "tens_y = torch.unsqueeze(tens_y, dim=0)\n",
    "print(tens_y.shape)\n",
    "tens_y = tens_y.float()\n",
    "\n",
    "test_y_zsc, mean_y, std_y = z_score_tensor(tens_y, fit=True)\n",
    "\n",
    "    # Create DFINE dataset objects and torch dataloaders\n",
    "test_trial = DFINEDataset(y=test_y_zsc)\n",
    "\n",
    "test_loader = DataLoader(test_trial, batch_size = 1, shuffle=False)\n",
    "\n",
    "test_latents = trainer.calculate_latents(train_loader=test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in gamble_latents['train'].keys():\n",
    "    gamble_avg = gamble_latents['train'][key].squeeze()\n",
    "    no_gamble_avg = ng_gamble_latents['train'][key].squeeze()\n",
    "    test_data = test_latents['batch_inference'][key]['train'].squeeze()\n",
    "    \n",
    "    data_list = [gamble_avg, no_gamble_avg, test_data]\n",
    "    label_list = [f\"average {key} of gamble\", f\"average {key} of no gamble\", f\"{key} of test data\"]\n",
    "\n",
    "    # Create subplots\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6), sharey=True)\n",
    "\n",
    "    for ax, label, data in zip(axes, label_list, data_list):\n",
    "        time = range(data.shape[0])  \n",
    "        dim_1 = data[:, 0]  \n",
    "        dim_2 = data[:, 1]  \n",
    "        dim_3 = data[:, 2]  \n",
    "\n",
    "        # Plot each dimension\n",
    "        ax.plot(time, dim_1, label=\"Dimension 1\")\n",
    "        ax.plot(time, dim_2, label=\"Dimension 2\")\n",
    "        ax.plot(time, dim_3, label=\"Dimension 3\")\n",
    "\n",
    "        ax.set_title(label)\n",
    "        ax.set_xlabel(\"Time\")\n",
    "        ax.grid()\n",
    "        ax.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    print(\"gamble dist:\")\n",
    "    g = findEuclidianDist(test_data, gamble_avg)\n",
    "    print(\"no gamble dist:\")\n",
    "    ng = findEuclidianDist(test_data, no_gamble_avg)\n",
    "    print(f'Actual result: {\"gamble\" if test_y_type.item() else \"no gamble\"} | Predicted result: {\"gamble\" if ng > g else \"no gamble\"}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4DJmQ-qXOxOV"
   },
   "source": [
    "## DFINE can learn the dynamics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EtHRuLztc_1a"
   },
   "source": [
    "Take avg of all gamble and safe trial (for low dimensional representation)\n",
    "in low dimensional space, and whichever we're closer to, we classify based on that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DL88wYYp0Sf8"
   },
   "source": [
    "Use x_pred when you need real-time state estimates, as this reflects the state based on the model and past observations up to the current time.\n",
    "Use x_smooth when you have access to the entire sequence of observations and can refine your state estimates retrospectively for better accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-965o0OfOxOV"
   },
   "source": [
    "Next, we plot the one-step ahead normalized root MSE (NRMSE) values of DFINE on the test data throughout the training vs. number of epochs, as well as that of the true system. The true system NRMSE is obtained using a UKF that uses the true system parameters (predictions are provided in the swiss_roll.pt datafile).\n",
    "\n",
    "Note that DFINE NRMSEs for the test (or validation) dataset are computed by valid_epoch function after every epoch while evaluating the model, and these values are bookkept under the trainer object. Please see TrainerDFINE.py."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l--1wBNj4L34"
   },
   "source": [
    "a_smooth: Smoothed latent variables during training and validation: use this, the manifold latent factor, as the low dimensional representation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "SiSEqnXP4Es6"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def euclidean_distance(point1, point2):\n",
    "    if len(point1) != len(point2):\n",
    "        raise ValueError(\"Both points must have the same number of dimensions\")\n",
    "\n",
    "    sum_squared_diff = 0\n",
    "    for i in range(len(point1)):\n",
    "        sum_squared_diff += (point1[i] - point2[i]) ** 2\n",
    "\n",
    "    return math.sqrt(sum_squared_diff)\n",
    "\n",
    "a_smooth = results['batch_inference']['a_smooth']['valid']\n",
    "y_res = results['batch_inference']['y_filter']['valid']\n",
    "print(a_smooth.shape)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KGEXICMDNyU0"
   },
   "source": [
    "3 phases: deliberation, anticipation, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "qDS1ij22OxOV"
   },
   "outputs": [],
   "source": [
    "def plot_nrmse_convergence(true_one_step_nrmse, dfine_one_step_nrmse_list, resolution=1):\n",
    "    # Downsample with desired resolution\n",
    "    num_epochs = len(dfine_one_step_nrmse_list) - 1\n",
    "    dfine_one_step_nrmse_list = dfine_one_step_nrmse_list[::resolution]\n",
    "    x = np.arange(0, len(dfine_one_step_nrmse_list))\n",
    "\n",
    "    # Plot the one-step-ahead prediction NRMSE of DFINE over the course of training, computed by valid_epoch function during model evaluation\n",
    "    f = plt.figure(figsize=(7,4))\n",
    "    ax = f.add_subplot(111)\n",
    "    plt.plot(x, torch.ones(len(dfine_one_step_nrmse_list))*true_one_step_nrmse, color='#239023', label='True model')\n",
    "    plt.plot(x, dfine_one_step_nrmse_list, color='#8D128D', label='Learned model')\n",
    "    plt.scatter(0, dfine_one_step_nrmse_list[0], s=100, color='#8D128D', marker='x', linewidth=2, label='Initialized model')\n",
    "\n",
    "    # Set the ticks\n",
    "    xticks = np.arange(0, num_epochs+1, resolution)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(xticks)\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('One-step-ahead \\nprediction error \\n(NRMSE)')\n",
    "    plt.legend()\n",
    "\n",
    "# One-step-ahead prediction with UKF by using the TRUE system\n",
    "_, true_test_one_step_nrmse = get_nrmse_error(test_y[:, 1:, :], test_true_y_pred)\n",
    "# DFINE NRMSEs on the test (or validation) dataset are computed throughout the training\n",
    "# during model evaluation and bookkept under the trainer object\n",
    "dfine_test_one_step_nrmse_list = trainer.training_valid_one_step_nrmses\n",
    "\n",
    "# Plot the NRMSE convergence over the course of training\n",
    "plot_nrmse_convergence(true_test_one_step_nrmse, dfine_test_one_step_nrmse_list, resolution=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lEbSoCoTOxOV"
   },
   "source": [
    "We see that DFINE’s one-step ahead NRMSE converges to that of the true model. Now, let's plot a trial of filtered trajectory inferred by DFINE in the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "V1GIQgLMOxOV"
   },
   "outputs": [],
   "source": [
    "# Filtering with DFINE\n",
    "dfine_test_y_filter_zsc = encoding_results['batch_inference']['y_filter']['valid']\n",
    "dfine_test_y_filter = dfine_test_y_filter_zsc * std_y + mean_y # reverting z-scoring\n",
    "\n",
    "# Plots the true and reconstructed trajectory on the Swiss-roll manifold\n",
    "def plot_reconst_traj_3D(y, y_hat, y_no_noise, trial):\n",
    "    num_samples = y.shape[1]\n",
    "    color_index = range(num_samples)\n",
    "    color_map = plt.cm.get_cmap('viridis')\n",
    "\n",
    "    xlim = [y[trial, :, 0].min(), y[trial, :, 0].max()]\n",
    "    ylim = [y[trial, :, 1].min(), y[trial, :, 1].max()]\n",
    "    zlim = [y[trial, :, 2].min(), y[trial, :, 2].max()]\n",
    "\n",
    "    f = plt.figure(figsize=(10,8))\n",
    "    ax = f.add_subplot(121, projection='3d')\n",
    "    ax_m = ax.scatter(y_no_noise[trial, :, 0].numpy(), y_no_noise[trial, :, 1].numpy(), y_no_noise[trial, :, 2].numpy(), c=color_index, vmin=0, vmax=num_samples, s=35, cmap=color_map)\n",
    "    f.colorbar(ax_m, fraction=0.046, pad=0.04, shrink=0.4, location='left')\n",
    "    ax.set_xlabel('Dim 1')\n",
    "    ax.set_ylabel('Dim 2')\n",
    "    ax.set_zlabel('Dim 3')\n",
    "    ax.set_xlim(xlim)\n",
    "    ax.set_ylim(ylim)\n",
    "    ax.set_zlim(zlim)\n",
    "    ax.set_title('True underlying trajectory')\n",
    "\n",
    "    ax = f.add_subplot(122, projection='3d')\n",
    "    ax_m = ax.scatter(y_hat[trial, :, 0].numpy(), y_hat[trial, :, 1].numpy(), y_hat[trial, :, 2].numpy(), c=color_index, vmin=0, vmax=num_samples, s=35, cmap=color_map)\n",
    "    ax.scatter(y[trial, :, 0].numpy(), y[trial, :, 1].numpy(), y[trial, :, 2].numpy(), color='#C0C0C0', alpha=0.4) # noisy observations\n",
    "    ax.set_xlabel('Dim 1')\n",
    "    ax.set_ylabel('Dim 2')\n",
    "    ax.set_zlabel('Dim 3')\n",
    "    ax.set_xlim(xlim)\n",
    "    ax.set_ylim(ylim)\n",
    "    ax.set_zlim(zlim)\n",
    "    title = 'DFINE Inferred trajectory (filtered)'\n",
    "    ax.set_title(title)\n",
    "    plt.show()\n",
    "\n",
    "# Plot DFINE inferred filtered observations\n",
    "plot_reconst_traj_3D(test_y, dfine_test_y_filter, test_y_no_noise, trial_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F855SDImOxOV"
   },
   "source": [
    "We see that DFINE’s inferred trajectory using causal filtering is nearly identical to the true trajectory. Note that the grey dots on the right are the noisy observations from which DFINE infers the underlying trajectory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Bih2gEUOxOV"
   },
   "source": [
    "## DFINE enables flexible causal and non-causal inference, even in the presence of missing observations.\n",
    "\n",
    "Next, we show DFINE's flexible inference performance, whether causally, non-causally, or even with missing observations.\n",
    "\n",
    "To show DFINE’s ability to handle missing observations, we define different sample dropping probabilities to randomly drop samples of observations over time. Based on these values, we create masks to mask the neural observation samples. We define the observed datapoint ratio as the ratio of the datapoints that are maintained/not-dropped to the total number of datapoints (or equivalently 1-(sample dropping probabilities)). This random drop was done to emulate the common problem of data drop in wireless neural interfaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "InHzAixiOxOV"
   },
   "outputs": [],
   "source": [
    "# DFINE Inference with missing samples\n",
    "p_drop_list = np.array([0, 0.1, 0.2, 0.25, 0.3, 0.4, 0.5, 0.6, 0.7, 0.75, 0.8, 0.9, 0.95]) # sample drop probability, samples are dropped uniformly across timesteps\n",
    "\n",
    "# Create masks for inference with missing samples\n",
    "def create_mask(num_trials, num_steps, p_drop=0):\n",
    "    keep_probs = torch.ones((num_trials, num_steps), dtype=torch.float32) * (1-p_drop)\n",
    "    mask = torch.bernoulli(keep_probs)\n",
    "    return mask.unsqueeze(dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ythd5wjnOxOV"
   },
   "source": [
    "For each sample dropping probability, we create corresponding masks and reinitialize the dataloaders with these masks. Then, we call 'trainer.save_encoding_results' function to perform inference with DFINE. Also, for each dropping probability, we compute NRMSEs both with filtering and smoothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "cdRY4aEQOxOW"
   },
   "outputs": [],
   "source": [
    "dfine_nrmse_filter_list, dfine_nrmse_smooth_list = [], []\n",
    "dfine_y_filter_list, dfine_y_smooth_list = [], []\n",
    "\n",
    "mask_list = []\n",
    "\n",
    "for p_drop in p_drop_list:\n",
    "    train_mask = create_mask(num_train_trials, num_steps, p_drop)\n",
    "    test_mask = create_mask(num_test_trials, num_steps, p_drop)\n",
    "    mask_list.append(test_mask)\n",
    "\n",
    "    # Create DFINE dataset objects and torch dataloaders\n",
    "    train_dataset_drop = DFINEDataset(y=train_y_zsc, mask=train_mask)\n",
    "    test_dataset_drop = DFINEDataset(y=test_y_zsc, mask=test_mask)\n",
    "\n",
    "    train_loader_drop = DataLoader(train_dataset_drop, batch_size=config.train.batch_size, shuffle=False)\n",
    "    test_loader_drop = DataLoader(test_dataset_drop, batch_size=config.train.batch_size, shuffle=False)\n",
    "\n",
    "    # Perform inference over the dataset with missing samples\n",
    "    encoding_results_drop = trainer.save_encoding_results(train_loader=train_loader_drop,\n",
    "                                                          valid_loader=test_loader_drop,\n",
    "                                                          save_results=False)\n",
    "\n",
    "    # Obtain DFINE inferred neural activity reconstructions (with filtering and smoothing) with missing samples\n",
    "    dfine_test_y_filter_zsc_drop = encoding_results_drop['batch_inference']['y_filter']['valid']\n",
    "    dfine_test_y_smooth_zsc_drop = encoding_results_drop['batch_inference']['y_smooth']['valid']\n",
    "\n",
    "    dfine_test_y_filter_drop = dfine_test_y_filter_zsc_drop * std_y + mean_y # reverting z-scoring\n",
    "    dfine_test_y_smooth_drop = dfine_test_y_smooth_zsc_drop * std_y + mean_y # reverting z-scoring\n",
    "\n",
    "    # Compute NRMSE of DFINE, filtering and smoothing\n",
    "    _, dfine_test_filter_nrmse = get_nrmse_error(test_y_no_noise, dfine_test_y_filter_drop)\n",
    "    _, dfine_test_smooth_nrmse = get_nrmse_error(test_y_no_noise, dfine_test_y_smooth_drop)\n",
    "\n",
    "    # Keep NRMSE stats and inferred variables in a list for plotting\n",
    "    dfine_nrmse_filter_list.append(dfine_test_filter_nrmse)\n",
    "    dfine_nrmse_smooth_list.append(dfine_test_smooth_nrmse)\n",
    "\n",
    "    dfine_y_filter_list.append(dfine_test_y_filter_drop)\n",
    "    dfine_y_smooth_list.append(dfine_test_y_smooth_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pvZmVpZkOxOW"
   },
   "source": [
    "Let's plot the DFINE NRMSE values obtained with filtering and smoothing over defined observed datapoint ratios (or 1-(sample dropping probabilities))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ilS7DkqHOxOX"
   },
   "source": [
    "We can see that the same trained DFINE model can perform both filtering and smoothing, and can do so with or without missing observations. Also, the difference between filtering and smoothing is larger in the low information regime, that is for low observed datapoint ratios.\n",
    "\n",
    "Finally, let's visualize the DFINE inferred filtered and smoothed trajectories for observed datapoint ratio of 0.1 (or sample dropping probability of 0.9)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "047xfPfHOxOX"
   },
   "outputs": [],
   "source": [
    "obs_ratios = [f'{1-i:.2f}' for i in p_drop_list]\n",
    "f = plt.figure()\n",
    "ax = f.add_subplot(111)\n",
    "plt.plot(np.arange(len(p_drop_list)), dfine_nrmse_filter_list[::-1], marker='.', label='Filtering', color='#008B8B')\n",
    "plt.plot(np.arange(len(p_drop_list)), dfine_nrmse_smooth_list[::-1], marker='.', label='Smoothing', color='#000080')\n",
    "ax.set_xticks(np.arange(len(p_drop_list)))\n",
    "ax.set_xticklabels(obs_ratios[::-1])\n",
    "ax.set_xlabel('Observed datapoint ratio')\n",
    "ax.set_ylabel('Underlying trajectory \\nreconstruction error \\n(NRMSE)')\n",
    "plt.xticks(rotation=30)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pS9Oa_y8OxOX"
   },
   "source": [
    "Note that the left-most plot is for visualization and shows that only 10% of the underlying trajectory will be observed in noisy neural observations. This means that only the noisy neural observations associated with the colored dots on the trajectory are used for inference, and all other neural observations (associated with all the other points of trajectory) are missing during inference."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
